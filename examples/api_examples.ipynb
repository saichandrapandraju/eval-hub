{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Eval Hub API Examples\"\n",
        "subtitle: \"Comprehensive guide to using the Evaluation Hub REST API\"\n",
        "author: \"Evaluation Service Team\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    code-fold: false\n",
        "    theme: cosmo\n",
        "  ipynb:\n",
        "    output-file: api_examples.ipynb\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Eval Hub API Examples\n",
        "\n",
        "This notebook demonstrates how to interact with the Evaluation Hub REST API running on `localhost:8000`.\n",
        "\n",
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from uuid import uuid4\n",
        "\n",
        "import requests\n",
        "\n",
        "# Configuration\n",
        "BASE_URL = \"http://localhost:8000\"\n",
        "API_BASE = f\"{BASE_URL}/api/v1\"\n",
        "\n",
        "# Helper function for pretty printing JSON responses\n",
        "def print_json(data):\n",
        "    print(json.dumps(data, indent=2, default=str))\n",
        "\n",
        "# Helper function for API requests\n",
        "def api_request(method: str, endpoint: str, **kwargs) -> requests.Response:\n",
        "    \"\"\"Make an API request with proper error handling.\"\"\"\n",
        "    url = f\"{API_BASE}{endpoint}\"\n",
        "    response = requests.request(method, url, **kwargs)\n",
        "\n",
        "    print(f\"{method.upper()} {url}\")\n",
        "    print(f\"Status: {response.status_code}\")\n",
        "\n",
        "    if response.headers.get('content-type', '').startswith('application/json'):\n",
        "        print(\"Response:\")\n",
        "        print_json(response.json())\n",
        "    else:\n",
        "        print(f\"Response: {response.text}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Health Check\n",
        "\n",
        "First, let's verify the service is running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/health\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"status\": \"healthy\",\n",
            "  \"version\": \"0.1.0\",\n",
            "  \"timestamp\": \"2025-11-08T23:18:29.974675\",\n",
            "  \"components\": {\n",
            "    \"mlflow\": {\n",
            "      \"status\": \"healthy\",\n",
            "      \"tracking_uri\": \"http://localhost:5000\"\n",
            "    }\n",
            "  },\n",
            "  \"uptime_seconds\": 1.1920928955078125e-06,\n",
            "  \"active_evaluations\": 0\n",
            "}\n",
            "--------------------------------------------------\n",
            "✅ Service is healthy!\n",
            "Version: 0.1.0\n",
            "Uptime: 0.0 seconds\n"
          ]
        }
      ],
      "source": [
        "response = api_request(\"GET\", \"/health\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    health_data = response.json()\n",
        "    print(\"✅ Service is healthy!\")\n",
        "    print(f\"Version: {health_data['version']}\")\n",
        "    print(f\"Uptime: {health_data['uptime_seconds']:.1f} seconds\")\n",
        "else:\n",
        "    print(\"❌ Service is not responding correctly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Provider Management\n",
        "\n",
        "### List All Providers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/providers\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"providers\": [\n",
            "    {\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"provider_name\": \"LM Evaluation Harness\",\n",
            "      \"description\": \"Comprehensive evaluation framework for language models with 167 benchmarks\",\n",
            "      \"provider_type\": \"builtin\",\n",
            "      \"benchmark_count\": 168\n",
            "    },\n",
            "    {\n",
            "      \"provider_id\": \"ragas\",\n",
            "      \"provider_name\": \"RAGAS\",\n",
            "      \"description\": \"Retrieval Augmented Generation Assessment framework\",\n",
            "      \"provider_type\": \"builtin\",\n",
            "      \"benchmark_count\": 4\n",
            "    },\n",
            "    {\n",
            "      \"provider_id\": \"garak\",\n",
            "      \"provider_name\": \"Garak\",\n",
            "      \"description\": \"LLM vulnerability scanner and red-teaming framework\",\n",
            "      \"provider_type\": \"builtin\",\n",
            "      \"benchmark_count\": 4\n",
            "    }\n",
            "  ],\n",
            "  \"total_providers\": 3,\n",
            "  \"total_benchmarks\": 176\n",
            "}\n",
            "--------------------------------------------------\n",
            "Found 3 providers:\n",
            "  - LM Evaluation Harness (lm_evaluation_harness)\n",
            "    Type: builtin\n",
            "    Benchmarks: 168\n",
            "  - RAGAS (ragas)\n",
            "    Type: builtin\n",
            "    Benchmarks: 4\n",
            "  - Garak (garak)\n",
            "    Type: builtin\n",
            "    Benchmarks: 4\n"
          ]
        }
      ],
      "source": [
        "response = api_request(\"GET\", \"/providers\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    providers_data = response.json()\n",
        "    print(f\"Found {providers_data['total_providers']} providers:\")\n",
        "    for provider in providers_data['providers']:\n",
        "        print(f\"  - {provider['provider_name']} ({provider['provider_id']})\")\n",
        "        print(f\"    Type: {provider['provider_type']}\")\n",
        "        print(f\"    Benchmarks: {provider['benchmark_count']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Specific Provider Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/providers/lm_evaluation_harness\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"provider_id\": \"lm_evaluation_harness\",\n",
            "  \"provider_name\": \"LM Evaluation Harness\",\n",
            "  \"description\": \"Comprehensive evaluation framework for language models with 167 benchmarks\",\n",
            "  \"provider_type\": \"builtin\",\n",
            "  \"benchmarks\": [\n",
            "    {\n",
            "      \"benchmark_id\": \"arc_easy\",\n",
            "      \"name\": \"ARC Easy\",\n",
            "      \"description\": \"ARC Easy evaluation benchmark - AI2 Reasoning Challenge (Easy)\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2376,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_boolq_lev\",\n",
            "      \"name\": \"Aradice Boolq Lev\",\n",
            "      \"description\": \"Aradice Boolq Lev evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3270,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp\",\n",
            "      \"name\": \"Blimp\",\n",
            "      \"description\": \"Blimp evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_anaphor_gender_agreement\",\n",
            "      \"name\": \"Blimp Anaphor Gender Agreement\",\n",
            "      \"description\": \"Blimp Anaphor Gender Agreement evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_animate_subject_trans\",\n",
            "      \"name\": \"Blimp Animate Subject Trans\",\n",
            "      \"description\": \"Blimp Animate Subject Trans evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_coordinate_structure_constraint_complex_left_branch\",\n",
            "      \"name\": \"Blimp Coordinate Structure Constraint Complex Left Branch\",\n",
            "      \"description\": \"Blimp Coordinate Structure Constraint Complex Left Branch evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_determiner_noun_agreement_2\",\n",
            "      \"name\": \"Blimp Determiner Noun Agreement 2\",\n",
            "      \"description\": \"Blimp Determiner Noun Agreement 2 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_determiner_noun_agreement_with_adj_2\",\n",
            "      \"name\": \"Blimp Determiner Noun Agreement With Adj 2\",\n",
            "      \"description\": \"Blimp Determiner Noun Agreement With Adj 2 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_determiner_noun_agreement_with_adjective_1\",\n",
            "      \"name\": \"Blimp Determiner Noun Agreement With Adjective 1\",\n",
            "      \"description\": \"Blimp Determiner Noun Agreement With Adjective 1 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_existential_there_object_raising\",\n",
            "      \"name\": \"Blimp Existential There Object Raising\",\n",
            "      \"description\": \"Blimp Existential There Object Raising evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_existential_there_subject_raising\",\n",
            "      \"name\": \"Blimp Existential There Subject Raising\",\n",
            "      \"description\": \"Blimp Existential There Subject Raising evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_intransitive\",\n",
            "      \"name\": \"Blimp Intransitive\",\n",
            "      \"description\": \"Blimp Intransitive evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_irregular_plural_subject_verb_agreement_1\",\n",
            "      \"name\": \"Blimp Irregular Plural Subject Verb Agreement 1\",\n",
            "      \"description\": \"Blimp Irregular Plural Subject Verb Agreement 1 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_left_branch_island_simple_question\",\n",
            "      \"name\": \"Blimp Left Branch Island Simple Question\",\n",
            "      \"description\": \"Blimp Left Branch Island Simple Question evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_npi_present_2\",\n",
            "      \"name\": \"Blimp Npi Present 2\",\n",
            "      \"description\": \"Blimp Npi Present 2 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_passive_1\",\n",
            "      \"name\": \"Blimp Passive 1\",\n",
            "      \"description\": \"Blimp Passive 1 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_humanities_history_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Humanities History Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Humanities History Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_humanities_philosophy_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Humanities Philosophy Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Humanities Philosophy Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_language_arabic-language_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Language Arabic-Language Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Language Arabic-Language Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_language_arabic-language_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_humanities_islamic-studies_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_language_arabic-language-general_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_other_driving-test_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Na Other Driving-Test Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Na Other Driving-Test Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_other_general-knowledge_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_language_arabic-language_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_other_management_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Univ Other Management Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Univ Other Management Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_openbookqa_eng\",\n",
            "      \"name\": \"Aradice Openbookqa Eng\",\n",
            "      \"description\": \"Aradice Openbookqa Eng evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_boolq\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Boolq\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Boolq evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3270,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_boolq_light\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Boolq Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Boolq Light evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3270,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_mt_boolq_light\",\n",
            "      \"name\": \"Arabic Mt Boolq Light\",\n",
            "      \"description\": \"Arabic Mt Boolq Light evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3270,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"leaderboard_bbh_salient_translation_error_detection\",\n",
            "      \"name\": \"Leaderboard Bbh Salient Translation Error Detection\",\n",
            "      \"description\": \"Leaderboard Bbh Salient Translation Error Detection evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"bleu\",\n",
            "        \"chrf\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"aclue_ancient_chinese_culture\",\n",
            "      \"name\": \"Aclue Ancient Chinese Culture\",\n",
            "      \"description\": \"Aclue Ancient Chinese Culture evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"african_flores\",\n",
            "      \"name\": \"African Flores\",\n",
            "      \"description\": \"African Flores evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"bleu\",\n",
            "        \"chrf\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli-irokobench\",\n",
            "      \"name\": \"Afrixnli-Irokobench\",\n",
            "      \"description\": \"Afrixnli-Irokobench evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli_amh_prompt_2\",\n",
            "      \"name\": \"Afrixnli Amh Prompt 2\",\n",
            "      \"description\": \"Afrixnli Amh Prompt 2 evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli_amh_prompt_5\",\n",
            "      \"name\": \"Afrixnli Amh Prompt 5\",\n",
            "      \"description\": \"Afrixnli Amh Prompt 5 evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli_en_direct_ewe\",\n",
            "      \"name\": \"Afrixnli En Direct Ewe\",\n",
            "      \"description\": \"Afrixnli En Direct Ewe evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli_en_direct_ibo\",\n",
            "      \"name\": \"Afrixnli En Direct Ibo\",\n",
            "      \"description\": \"Afrixnli En Direct Ibo evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli_en_direct_lug\",\n",
            "      \"name\": \"Afrixnli En Direct Lug\",\n",
            "      \"description\": \"Afrixnli En Direct Lug evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli_en_direct_sot\",\n",
            "      \"name\": \"Afrixnli En Direct Sot\",\n",
            "      \"description\": \"Afrixnli En Direct Sot evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli_en_direct_wol\",\n",
            "      \"name\": \"Afrixnli En Direct Wol\",\n",
            "      \"description\": \"Afrixnli En Direct Wol evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"afrixnli_en_direct_zul\",\n",
            "      \"name\": \"Afrixnli En Direct Zul\",\n",
            "      \"description\": \"Afrixnli En Direct Zul evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_stem_math_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Stem Math Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_college_mathematics_light\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_high_school_mathematics\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"cmmlu_college_mathematics\",\n",
            "      \"name\": \"Cmmlu College Mathematics\",\n",
            "      \"description\": \"Cmmlu College Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"cmmlu_high_school_mathematics\",\n",
            "      \"name\": \"Cmmlu High School Mathematics\",\n",
            "      \"description\": \"Cmmlu High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_am_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full Am High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Am High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_ar_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full Ar High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Ar High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_bn_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full Bn High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Bn High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_cs_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full Cs High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Cs High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_de_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full De High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full De High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_el_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full El High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full El High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_en_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full En High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full En High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_es_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full Es High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Es High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_fa_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full Fa High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Fa High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_fil_high_school_mathematics\",\n",
            "      \"name\": \"Global Mmlu Full Fil High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Fil High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_piqa_lev\",\n",
            "      \"name\": \"Aradice Piqa Lev\",\n",
            "      \"description\": \"Aradice Piqa Lev evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1838,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_winogrande_eng\",\n",
            "      \"name\": \"Aradice Winogrande Eng\",\n",
            "      \"description\": \"Aradice Winogrande Eng evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1267,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_copa\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Copa\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Copa evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_copa_light\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Copa Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Copa Light evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_hellaswag\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_hellaswag_light\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag Light evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_piqa\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Piqa\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Piqa evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1838,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_piqa_light\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Piqa Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Piqa Light evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1838,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_mt_hellaswag\",\n",
            "      \"name\": \"Arabic Mt Hellaswag\",\n",
            "      \"description\": \"Arabic Mt Hellaswag evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_mt_piqa\",\n",
            "      \"name\": \"Arabic Mt Piqa\",\n",
            "      \"description\": \"Arabic Mt Piqa evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1838,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"copa_ar\",\n",
            "      \"name\": \"Copa Ar\",\n",
            "      \"description\": \"Copa Ar evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"copal_id_colloquial\",\n",
            "      \"name\": \"Copal Id Colloquial\",\n",
            "      \"description\": \"Copal Id Colloquial evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"darijahellaswag\",\n",
            "      \"name\": \"Darijahellaswag\",\n",
            "      \"description\": \"Darijahellaswag evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"egyhellaswag\",\n",
            "      \"name\": \"Egyhellaswag\",\n",
            "      \"description\": \"Egyhellaswag evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"hellaswag_ar\",\n",
            "      \"name\": \"Hellaswag Ar\",\n",
            "      \"description\": \"Hellaswag Ar evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_race\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Race\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Race evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 674,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_race_light\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Race Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Race Light evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 674,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_mt_race_light\",\n",
            "      \"name\": \"Arabic Mt Race Light\",\n",
            "      \"description\": \"Arabic Mt Race Light evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 674,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"blimp_drop_argument\",\n",
            "      \"name\": \"Blimp Drop Argument\",\n",
            "      \"description\": \"Blimp Drop Argument evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 9536,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bigbench_gre_reading_comprehension_multiple_choice\",\n",
            "      \"name\": \"Bigbench Gre Reading Comprehension Multiple Choice\",\n",
            "      \"description\": \"Bigbench Gre Reading Comprehension Multiple Choice evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"eus_reading\",\n",
            "      \"name\": \"Eus Reading\",\n",
            "      \"description\": \"Eus Reading evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"longbench_qasper\",\n",
            "      \"name\": \"Longbench Qasper\",\n",
            "      \"description\": \"Longbench Qasper evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"qasper_freeform\",\n",
            "      \"name\": \"Qasper Freeform\",\n",
            "      \"description\": \"Qasper Freeform evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"ruler_qa_squad\",\n",
            "      \"name\": \"Ruler Qa Squad\",\n",
            "      \"description\": \"Ruler Qa Squad evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"scrolls_qasper\",\n",
            "      \"name\": \"Scrolls Qasper\",\n",
            "      \"description\": \"Scrolls Qasper evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_social-science_economics_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Social-Science Economics Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Social-Science Economics Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_social-science_geography_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Social-Science Geography Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Social-Science Geography Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_stem_computer-science_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Stem Computer-Science Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Stem Computer-Science Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_stem_physics_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Stem Physics Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Stem Physics Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_civics_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_economics_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_social-science_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_stem_computer-science_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_social-science_geography_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_social-science_social-science_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_stem_natural-science_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_social-science_accounting_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_social-science_political-science_egy\",\n",
            "      \"name\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_stem_computer-science_lev\",\n",
            "      \"name\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_college_biology_light\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Biology Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Biology Light evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"agieval_logiqa_zh\",\n",
            "      \"name\": \"Agieval Logiqa Zh\",\n",
            "      \"description\": \"Agieval Logiqa Zh evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 651,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh\",\n",
            "      \"name\": \"Bbh\",\n",
            "      \"description\": \"Bbh evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot\",\n",
            "      \"name\": \"Bbh Cot Fewshot\",\n",
            "      \"description\": \"Bbh Cot Fewshot evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_causal_judgement\",\n",
            "      \"name\": \"Bbh Cot Fewshot Causal Judgement\",\n",
            "      \"description\": \"Bbh Cot Fewshot Causal Judgement evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_dyck_languages\",\n",
            "      \"name\": \"Bbh Cot Fewshot Dyck Languages\",\n",
            "      \"description\": \"Bbh Cot Fewshot Dyck Languages evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_hyperbaton\",\n",
            "      \"name\": \"Bbh Cot Fewshot Hyperbaton\",\n",
            "      \"description\": \"Bbh Cot Fewshot Hyperbaton evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_logical_deduction_three_objects\",\n",
            "      \"name\": \"Bbh Cot Fewshot Logical Deduction Three Objects\",\n",
            "      \"description\": \"Bbh Cot Fewshot Logical Deduction Three Objects evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_navigate\",\n",
            "      \"name\": \"Bbh Cot Fewshot Navigate\",\n",
            "      \"description\": \"Bbh Cot Fewshot Navigate evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_reasoning_about_colored_objects\",\n",
            "      \"name\": \"Bbh Cot Fewshot Reasoning About Colored Objects\",\n",
            "      \"description\": \"Bbh Cot Fewshot Reasoning About Colored Objects evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_snarks\",\n",
            "      \"name\": \"Bbh Cot Fewshot Snarks\",\n",
            "      \"description\": \"Bbh Cot Fewshot Snarks evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_tracking_shuffled_objects_five_objects\",\n",
            "      \"name\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects\",\n",
            "      \"description\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_fewshot_web_of_lies\",\n",
            "      \"name\": \"Bbh Cot Fewshot Web Of Lies\",\n",
            "      \"description\": \"Bbh Cot Fewshot Web Of Lies evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_zeroshot\",\n",
            "      \"name\": \"Bbh Cot Zeroshot\",\n",
            "      \"description\": \"Bbh Cot Zeroshot evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_zeroshot_causal_judgement\",\n",
            "      \"name\": \"Bbh Cot Zeroshot Causal Judgement\",\n",
            "      \"description\": \"Bbh Cot Zeroshot Causal Judgement evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bbh_cot_zeroshot_dyck_languages\",\n",
            "      \"name\": \"Bbh Cot Zeroshot Dyck Languages\",\n",
            "      \"description\": \"Bbh Cot Zeroshot Dyck Languages evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_anatomy\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu Anatomy\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu Anatomy evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_clinical_knowledge\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_medical_genetics\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_professional_medicine\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"cmmlu_professional_medicine\",\n",
            "      \"name\": \"Cmmlu Professional Medicine\",\n",
            "      \"description\": \"Cmmlu Professional Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"cmmlu_traditional_chinese_medicine\",\n",
            "      \"name\": \"Cmmlu Traditional Chinese Medicine\",\n",
            "      \"description\": \"Cmmlu Traditional Chinese Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_am_anatomy\",\n",
            "      \"name\": \"Global Mmlu Full Am Anatomy\",\n",
            "      \"description\": \"Global Mmlu Full Am Anatomy evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_am_clinical_knowledge\",\n",
            "      \"name\": \"Global Mmlu Full Am Clinical Knowledge\",\n",
            "      \"description\": \"Global Mmlu Full Am Clinical Knowledge evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_am_medical_genetics\",\n",
            "      \"name\": \"Global Mmlu Full Am Medical Genetics\",\n",
            "      \"description\": \"Global Mmlu Full Am Medical Genetics evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_am_professional_medicine\",\n",
            "      \"name\": \"Global Mmlu Full Am Professional Medicine\",\n",
            "      \"description\": \"Global Mmlu Full Am Professional Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_ar_anatomy\",\n",
            "      \"name\": \"Global Mmlu Full Ar Anatomy\",\n",
            "      \"description\": \"Global Mmlu Full Ar Anatomy evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_ar_clinical_knowledge\",\n",
            "      \"name\": \"Global Mmlu Full Ar Clinical Knowledge\",\n",
            "      \"description\": \"Global Mmlu Full Ar Clinical Knowledge evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_ar_medical_genetics\",\n",
            "      \"name\": \"Global Mmlu Full Ar Medical Genetics\",\n",
            "      \"description\": \"Global Mmlu Full Ar Medical Genetics evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_ar_professional_medicine\",\n",
            "      \"name\": \"Global Mmlu Full Ar Professional Medicine\",\n",
            "      \"description\": \"Global Mmlu Full Ar Professional Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"global_mmlu_full_bn_anatomy\",\n",
            "      \"name\": \"Global Mmlu Full Bn Anatomy\",\n",
            "      \"description\": \"Global Mmlu Full Bn Anatomy evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lambada_openai\",\n",
            "      \"name\": \"Lambada Openai\",\n",
            "      \"description\": \"Lambada Openai evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lambada_openai_mt_en\",\n",
            "      \"name\": \"Lambada Openai Mt En\",\n",
            "      \"description\": \"Lambada Openai Mt En evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lambada_openai_mt_it\",\n",
            "      \"name\": \"Lambada Openai Mt It\",\n",
            "      \"description\": \"Lambada Openai Mt It evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lambada_openai_mt_stablelm_es\",\n",
            "      \"name\": \"Lambada Openai Mt Stablelm Es\",\n",
            "      \"description\": \"Lambada Openai Mt Stablelm Es evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lambada_openai_mt_stablelm_nl\",\n",
            "      \"name\": \"Lambada Openai Mt Stablelm Nl\",\n",
            "      \"description\": \"Lambada Openai Mt Stablelm Nl evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lambada_standard_cloze_yaml\",\n",
            "      \"name\": \"Lambada Standard Cloze Yaml\",\n",
            "      \"description\": \"Lambada Standard Cloze Yaml evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"paloma_wikitext_103\",\n",
            "      \"name\": \"Paloma Wikitext 103\",\n",
            "      \"description\": \"Paloma Wikitext 103 evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 4358,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"pile_arxiv\",\n",
            "      \"name\": \"Pile Arxiv\",\n",
            "      \"description\": \"Pile Arxiv evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"pile_freelaw\",\n",
            "      \"name\": \"Pile Freelaw\",\n",
            "      \"description\": \"Pile Freelaw evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"pile_hackernews\",\n",
            "      \"name\": \"Pile Hackernews\",\n",
            "      \"description\": \"Pile Hackernews evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"pile_openwebtext2\",\n",
            "      \"name\": \"Pile Openwebtext2\",\n",
            "      \"description\": \"Pile Openwebtext2 evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"pile_ubuntu-irc\",\n",
            "      \"name\": \"Pile Ubuntu-Irc\",\n",
            "      \"description\": \"Pile Ubuntu-Irc evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"pile_youtubesubtitles\",\n",
            "      \"name\": \"Pile Youtubesubtitles\",\n",
            "      \"description\": \"Pile Youtubesubtitles evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"wikitext\",\n",
            "      \"name\": \"Wikitext\",\n",
            "      \"description\": \"Wikitext evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 4358,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"careqa_open_perplexity\",\n",
            "      \"name\": \"Careqa Open Perplexity\",\n",
            "      \"description\": \"Careqa Open Perplexity evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"AraDiCE_truthfulqa_mc1_lev\",\n",
            "      \"name\": \"Aradice Truthfulqa Mc1 Lev\",\n",
            "      \"description\": \"Aradice Truthfulqa Mc1 Lev evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"metabench_truthfulqa_permute\",\n",
            "      \"name\": \"Metabench Truthfulqa Permute\",\n",
            "      \"description\": \"Metabench Truthfulqa Permute evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"nortruthfulqa_gen_nno_p0\",\n",
            "      \"name\": \"Nortruthfulqa Gen Nno P0\",\n",
            "      \"description\": \"Nortruthfulqa Gen Nno P0 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"nortruthfulqa_gen_nno_p3\",\n",
            "      \"name\": \"Nortruthfulqa Gen Nno P3\",\n",
            "      \"description\": \"Nortruthfulqa Gen Nno P3 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"nortruthfulqa_gen_nob_p1\",\n",
            "      \"name\": \"Nortruthfulqa Gen Nob P1\",\n",
            "      \"description\": \"Nortruthfulqa Gen Nob P1 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"nortruthfulqa_gen_nob_p4\",\n",
            "      \"name\": \"Nortruthfulqa Gen Nob P4\",\n",
            "      \"description\": \"Nortruthfulqa Gen Nob P4 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"nortruthfulqa_mc_nno_p2\",\n",
            "      \"name\": \"Nortruthfulqa Mc Nno P2\",\n",
            "      \"description\": \"Nortruthfulqa Mc Nno P2 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"nortruthfulqa_mc_nob_p0\",\n",
            "      \"name\": \"Nortruthfulqa Mc Nob P0\",\n",
            "      \"description\": \"Nortruthfulqa Mc Nob P0 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"nortruthfulqa_mc_nob_p3\",\n",
            "      \"name\": \"Nortruthfulqa Mc Nob P3\",\n",
            "      \"description\": \"Nortruthfulqa Mc Nob P3 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"tinyTruthfulQA\",\n",
            "      \"name\": \"Tinytruthfulqa\",\n",
            "      \"description\": \"Tinytruthfulqa evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"truthfulqa-multi_gen_ca\",\n",
            "      \"name\": \"Truthfulqa-Multi Gen Ca\",\n",
            "      \"description\": \"Truthfulqa-Multi Gen Ca evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"truthfulqa-multi_gen_eu\",\n",
            "      \"name\": \"Truthfulqa-Multi Gen Eu\",\n",
            "      \"description\": \"Truthfulqa-Multi Gen Eu evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"truthfulqa-multi_mc1_en\",\n",
            "      \"name\": \"Truthfulqa-Multi Mc1 En\",\n",
            "      \"description\": \"Truthfulqa-Multi Mc1 En evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"truthfulqa-multi_mc1_gl\",\n",
            "      \"name\": \"Truthfulqa-Multi Mc1 Gl\",\n",
            "      \"description\": \"Truthfulqa-Multi Mc1 Gl evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"truthfulqa-multi_mc2_es\",\n",
            "      \"name\": \"Truthfulqa-Multi Mc2 Es\",\n",
            "      \"description\": \"Truthfulqa-Multi Mc2 Es evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"bigbench_code_line_description_multiple_choice\",\n",
            "      \"name\": \"Bigbench Code Line Description Multiple Choice\",\n",
            "      \"description\": \"Bigbench Code Line Description Multiple Choice evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"ceval-valid_college_programming\",\n",
            "      \"name\": \"Ceval-Valid College Programming\",\n",
            "      \"description\": \"Ceval-Valid College Programming evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"code2text_javascript\",\n",
            "      \"name\": \"Code2Text Javascript\",\n",
            "      \"description\": \"Code2Text Javascript evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"code2text_ruby\",\n",
            "      \"name\": \"Code2Text Ruby\",\n",
            "      \"description\": \"Code2Text Ruby evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"humaneval\",\n",
            "      \"name\": \"Humaneval\",\n",
            "      \"description\": \"Humaneval evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"humaneval_instruct\",\n",
            "      \"name\": \"Humaneval Instruct\",\n",
            "      \"description\": \"Humaneval Instruct evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"mbpp_instruct\",\n",
            "      \"name\": \"Mbpp Instruct\",\n",
            "      \"description\": \"Mbpp Instruct evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "--------------------------------------------------\n",
            "Provider: LM Evaluation Harness\n",
            "Description: Comprehensive evaluation framework for language models with 167 benchmarks\n",
            "Number of benchmarks: 168\n"
          ]
        }
      ],
      "source": [
        "# Get details for the lm_evaluation_harness provider\n",
        "provider_id = \"lm_evaluation_harness\"\n",
        "response = api_request(\"GET\", f\"/providers/{provider_id}\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    provider = response.json()\n",
        "    print(f\"Provider: {provider['provider_name']}\")\n",
        "    print(f\"Description: {provider['description']}\")\n",
        "    print(f\"Number of benchmarks: {len(provider['benchmarks'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark Discovery\n",
        "\n",
        "### List All Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/benchmarks\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"benchmarks\": [\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arc_easy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"ARC Easy\",\n",
            "      \"description\": \"ARC Easy evaluation benchmark - AI2 Reasoning Challenge (Easy)\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2376,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_boolq_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Boolq Lev\",\n",
            "      \"description\": \"Aradice Boolq Lev evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3270,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp\",\n",
            "      \"description\": \"Blimp evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_anaphor_gender_agreement\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Anaphor Gender Agreement\",\n",
            "      \"description\": \"Blimp Anaphor Gender Agreement evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_animate_subject_trans\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Animate Subject Trans\",\n",
            "      \"description\": \"Blimp Animate Subject Trans evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_coordinate_structure_constraint_complex_left_branch\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Coordinate Structure Constraint Complex Left Branch\",\n",
            "      \"description\": \"Blimp Coordinate Structure Constraint Complex Left Branch evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_determiner_noun_agreement_2\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Determiner Noun Agreement 2\",\n",
            "      \"description\": \"Blimp Determiner Noun Agreement 2 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_determiner_noun_agreement_with_adj_2\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Determiner Noun Agreement With Adj 2\",\n",
            "      \"description\": \"Blimp Determiner Noun Agreement With Adj 2 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_determiner_noun_agreement_with_adjective_1\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Determiner Noun Agreement With Adjective 1\",\n",
            "      \"description\": \"Blimp Determiner Noun Agreement With Adjective 1 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_existential_there_object_raising\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Existential There Object Raising\",\n",
            "      \"description\": \"Blimp Existential There Object Raising evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_existential_there_subject_raising\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Existential There Subject Raising\",\n",
            "      \"description\": \"Blimp Existential There Subject Raising evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_intransitive\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Intransitive\",\n",
            "      \"description\": \"Blimp Intransitive evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_irregular_plural_subject_verb_agreement_1\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Irregular Plural Subject Verb Agreement 1\",\n",
            "      \"description\": \"Blimp Irregular Plural Subject Verb Agreement 1 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_left_branch_island_simple_question\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Left Branch Island Simple Question\",\n",
            "      \"description\": \"Blimp Left Branch Island Simple Question evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_npi_present_2\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Npi Present 2\",\n",
            "      \"description\": \"Blimp Npi Present 2 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_passive_1\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Passive 1\",\n",
            "      \"description\": \"Blimp Passive 1 evaluation benchmark\",\n",
            "      \"category\": \"general\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"general\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_humanities_history_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Humanities History Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Humanities History Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_humanities_philosophy_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Humanities Philosophy Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Humanities Philosophy Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_language_arabic-language_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Language Arabic-Language Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Language Arabic-Language Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_language_arabic-language_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_na_humanities_islamic-studies_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_na_language_arabic-language-general_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_na_other_driving-test_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Na Other Driving-Test Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Na Other Driving-Test Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_na_other_general-knowledge_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_language_arabic-language_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_univ_other_management_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Univ Other Management Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Univ Other Management Egy evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_openbookqa_eng\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Openbookqa Eng\",\n",
            "      \"description\": \"Aradice Openbookqa Eng evaluation benchmark\",\n",
            "      \"category\": \"knowledge\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"knowledge\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_boolq\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Boolq\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Boolq evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3270,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_boolq_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Boolq Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Boolq Light evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3270,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_mt_boolq_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Mt Boolq Light\",\n",
            "      \"description\": \"Arabic Mt Boolq Light evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3270,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::leaderboard_bbh_salient_translation_error_detection\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Leaderboard Bbh Salient Translation Error Detection\",\n",
            "      \"description\": \"Leaderboard Bbh Salient Translation Error Detection evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"bleu\",\n",
            "        \"chrf\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::aclue_ancient_chinese_culture\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aclue Ancient Chinese Culture\",\n",
            "      \"description\": \"Aclue Ancient Chinese Culture evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::african_flores\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"African Flores\",\n",
            "      \"description\": \"African Flores evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"bleu\",\n",
            "        \"chrf\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli-irokobench\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli-Irokobench\",\n",
            "      \"description\": \"Afrixnli-Irokobench evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_amh_prompt_2\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli Amh Prompt 2\",\n",
            "      \"description\": \"Afrixnli Amh Prompt 2 evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_amh_prompt_5\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli Amh Prompt 5\",\n",
            "      \"description\": \"Afrixnli Amh Prompt 5 evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_ewe\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli En Direct Ewe\",\n",
            "      \"description\": \"Afrixnli En Direct Ewe evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_ibo\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli En Direct Ibo\",\n",
            "      \"description\": \"Afrixnli En Direct Ibo evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_lug\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli En Direct Lug\",\n",
            "      \"description\": \"Afrixnli En Direct Lug evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_sot\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli En Direct Sot\",\n",
            "      \"description\": \"Afrixnli En Direct Sot evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_wol\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli En Direct Wol\",\n",
            "      \"description\": \"Afrixnli En Direct Wol evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_zul\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Afrixnli En Direct Zul\",\n",
            "      \"description\": \"Afrixnli En Direct Zul evaluation benchmark\",\n",
            "      \"category\": \"multilingual\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 2000,\n",
            "      \"tags\": [\n",
            "        \"multilingual\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_stem_math_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Stem Math Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_college_mathematics_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_college_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Cmmlu College Mathematics\",\n",
            "      \"description\": \"Cmmlu College Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Cmmlu High School Mathematics\",\n",
            "      \"description\": \"Cmmlu High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Am High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Am High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Ar High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Ar High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_bn_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Bn High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Bn High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_cs_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Cs High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Cs High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_de_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full De High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full De High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_el_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full El High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full El High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_en_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full En High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full En High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_es_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Es High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Es High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_fa_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Fa High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Fa High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_fil_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Fil High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Fil High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_piqa_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Piqa Lev\",\n",
            "      \"description\": \"Aradice Piqa Lev evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1838,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_winogrande_eng\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Winogrande Eng\",\n",
            "      \"description\": \"Aradice Winogrande Eng evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1267,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_copa\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Copa\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Copa evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_copa_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Copa Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Copa Light evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_hellaswag\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_hellaswag_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag Light evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_piqa\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Piqa\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Piqa evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1838,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_piqa_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Piqa Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Piqa Light evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1838,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_mt_hellaswag\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Mt Hellaswag\",\n",
            "      \"description\": \"Arabic Mt Hellaswag evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_mt_piqa\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Mt Piqa\",\n",
            "      \"description\": \"Arabic Mt Piqa evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1838,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::copa_ar\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Copa Ar\",\n",
            "      \"description\": \"Copa Ar evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::copal_id_colloquial\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Copal Id Colloquial\",\n",
            "      \"description\": \"Copal Id Colloquial evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::darijahellaswag\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Darijahellaswag\",\n",
            "      \"description\": \"Darijahellaswag evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::egyhellaswag\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Egyhellaswag\",\n",
            "      \"description\": \"Egyhellaswag evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::hellaswag_ar\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Hellaswag Ar\",\n",
            "      \"description\": \"Hellaswag Ar evaluation benchmark\",\n",
            "      \"category\": \"reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10042,\n",
            "      \"tags\": [\n",
            "        \"reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_race\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Race\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Race evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 674,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_race_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mt Race Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mt Race Light evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 674,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_mt_race_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Mt Race Light\",\n",
            "      \"description\": \"Arabic Mt Race Light evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 674,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::blimp_drop_argument\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Blimp Drop Argument\",\n",
            "      \"description\": \"Blimp Drop Argument evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 9536,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bigbench_gre_reading_comprehension_multiple_choice\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bigbench Gre Reading Comprehension Multiple Choice\",\n",
            "      \"description\": \"Bigbench Gre Reading Comprehension Multiple Choice evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::eus_reading\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Eus Reading\",\n",
            "      \"description\": \"Eus Reading evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::longbench_qasper\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Longbench Qasper\",\n",
            "      \"description\": \"Longbench Qasper evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::qasper_freeform\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Qasper Freeform\",\n",
            "      \"description\": \"Qasper Freeform evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::ruler_qa_squad\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Ruler Qa Squad\",\n",
            "      \"description\": \"Ruler Qa Squad evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::scrolls_qasper\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Scrolls Qasper\",\n",
            "      \"description\": \"Scrolls Qasper evaluation benchmark\",\n",
            "      \"category\": \"reading_comprehension\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 3000,\n",
            "      \"tags\": [\n",
            "        \"reading_comprehension\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_social-science_economics_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Social-Science Economics Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Social-Science Economics Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_social-science_geography_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Social-Science Geography Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Social-Science Geography Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_stem_computer-science_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Stem Computer-Science Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Stem Computer-Science Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_stem_physics_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu High Stem Physics Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu High Stem Physics Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_social-science_civics_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_social-science_economics_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_social-science_social-science_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_stem_computer-science_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_social-science_geography_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_social-science_social-science_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_stem_natural-science_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_univ_social-science_accounting_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_univ_social-science_political-science_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_univ_stem_computer-science_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev\",\n",
            "      \"description\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_college_biology_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Biology Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Biology Light evaluation benchmark\",\n",
            "      \"category\": \"science\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"science\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::agieval_logiqa_zh\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Agieval Logiqa Zh\",\n",
            "      \"description\": \"Agieval Logiqa Zh evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 651,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh\",\n",
            "      \"description\": \"Bbh evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot\",\n",
            "      \"description\": \"Bbh Cot Fewshot evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_causal_judgement\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Causal Judgement\",\n",
            "      \"description\": \"Bbh Cot Fewshot Causal Judgement evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_dyck_languages\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Dyck Languages\",\n",
            "      \"description\": \"Bbh Cot Fewshot Dyck Languages evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_hyperbaton\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Hyperbaton\",\n",
            "      \"description\": \"Bbh Cot Fewshot Hyperbaton evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_logical_deduction_three_objects\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Logical Deduction Three Objects\",\n",
            "      \"description\": \"Bbh Cot Fewshot Logical Deduction Three Objects evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_navigate\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Navigate\",\n",
            "      \"description\": \"Bbh Cot Fewshot Navigate evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_reasoning_about_colored_objects\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Reasoning About Colored Objects\",\n",
            "      \"description\": \"Bbh Cot Fewshot Reasoning About Colored Objects evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_snarks\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Snarks\",\n",
            "      \"description\": \"Bbh Cot Fewshot Snarks evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_tracking_shuffled_objects_five_objects\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects\",\n",
            "      \"description\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_web_of_lies\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Fewshot Web Of Lies\",\n",
            "      \"description\": \"Bbh Cot Fewshot Web Of Lies evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 5,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_zeroshot\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Zeroshot\",\n",
            "      \"description\": \"Bbh Cot Zeroshot evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_zeroshot_causal_judgement\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Zeroshot Causal Judgement\",\n",
            "      \"description\": \"Bbh Cot Zeroshot Causal Judgement evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_zeroshot_dyck_languages\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bbh Cot Zeroshot Dyck Languages\",\n",
            "      \"description\": \"Bbh Cot Zeroshot Dyck Languages evaluation benchmark\",\n",
            "      \"category\": \"logic_reasoning\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"logic_reasoning\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_anatomy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu Anatomy\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu Anatomy evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_clinical_knowledge\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_medical_genetics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_professional_medicine\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_professional_medicine\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Cmmlu Professional Medicine\",\n",
            "      \"description\": \"Cmmlu Professional Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_traditional_chinese_medicine\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Cmmlu Traditional Chinese Medicine\",\n",
            "      \"description\": \"Cmmlu Traditional Chinese Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_anatomy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Am Anatomy\",\n",
            "      \"description\": \"Global Mmlu Full Am Anatomy evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_clinical_knowledge\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Am Clinical Knowledge\",\n",
            "      \"description\": \"Global Mmlu Full Am Clinical Knowledge evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_medical_genetics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Am Medical Genetics\",\n",
            "      \"description\": \"Global Mmlu Full Am Medical Genetics evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_professional_medicine\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Am Professional Medicine\",\n",
            "      \"description\": \"Global Mmlu Full Am Professional Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_anatomy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Ar Anatomy\",\n",
            "      \"description\": \"Global Mmlu Full Ar Anatomy evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_clinical_knowledge\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Ar Clinical Knowledge\",\n",
            "      \"description\": \"Global Mmlu Full Ar Clinical Knowledge evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_medical_genetics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Ar Medical Genetics\",\n",
            "      \"description\": \"Global Mmlu Full Ar Medical Genetics evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_professional_medicine\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Ar Professional Medicine\",\n",
            "      \"description\": \"Global Mmlu Full Ar Professional Medicine evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_bn_anatomy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Bn Anatomy\",\n",
            "      \"description\": \"Global Mmlu Full Bn Anatomy evaluation benchmark\",\n",
            "      \"category\": \"medical\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"medical\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Lambada Openai\",\n",
            "      \"description\": \"Lambada Openai evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai_mt_en\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Lambada Openai Mt En\",\n",
            "      \"description\": \"Lambada Openai Mt En evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai_mt_it\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Lambada Openai Mt It\",\n",
            "      \"description\": \"Lambada Openai Mt It evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai_mt_stablelm_es\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Lambada Openai Mt Stablelm Es\",\n",
            "      \"description\": \"Lambada Openai Mt Stablelm Es evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai_mt_stablelm_nl\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Lambada Openai Mt Stablelm Nl\",\n",
            "      \"description\": \"Lambada Openai Mt Stablelm Nl evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::lambada_standard_cloze_yaml\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Lambada Standard Cloze Yaml\",\n",
            "      \"description\": \"Lambada Standard Cloze Yaml evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"perplexity\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 5153,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::paloma_wikitext_103\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Paloma Wikitext 103\",\n",
            "      \"description\": \"Paloma Wikitext 103 evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 4358,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::pile_arxiv\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Pile Arxiv\",\n",
            "      \"description\": \"Pile Arxiv evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::pile_freelaw\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Pile Freelaw\",\n",
            "      \"description\": \"Pile Freelaw evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::pile_hackernews\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Pile Hackernews\",\n",
            "      \"description\": \"Pile Hackernews evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::pile_openwebtext2\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Pile Openwebtext2\",\n",
            "      \"description\": \"Pile Openwebtext2 evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::pile_ubuntu-irc\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Pile Ubuntu-Irc\",\n",
            "      \"description\": \"Pile Ubuntu-Irc evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::pile_youtubesubtitles\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Pile Youtubesubtitles\",\n",
            "      \"description\": \"Pile Youtubesubtitles evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 210000000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::wikitext\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Wikitext\",\n",
            "      \"description\": \"Wikitext evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 4358,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::careqa_open_perplexity\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Careqa Open Perplexity\",\n",
            "      \"description\": \"Careqa Open Perplexity evaluation benchmark\",\n",
            "      \"category\": \"language_modeling\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 10000,\n",
            "      \"tags\": [\n",
            "        \"language_modeling\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_truthfulqa_mc1_lev\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Truthfulqa Mc1 Lev\",\n",
            "      \"description\": \"Aradice Truthfulqa Mc1 Lev evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::metabench_truthfulqa_permute\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Metabench Truthfulqa Permute\",\n",
            "      \"description\": \"Metabench Truthfulqa Permute evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_gen_nno_p0\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Nortruthfulqa Gen Nno P0\",\n",
            "      \"description\": \"Nortruthfulqa Gen Nno P0 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_gen_nno_p3\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Nortruthfulqa Gen Nno P3\",\n",
            "      \"description\": \"Nortruthfulqa Gen Nno P3 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_gen_nob_p1\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Nortruthfulqa Gen Nob P1\",\n",
            "      \"description\": \"Nortruthfulqa Gen Nob P1 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_gen_nob_p4\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Nortruthfulqa Gen Nob P4\",\n",
            "      \"description\": \"Nortruthfulqa Gen Nob P4 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_mc_nno_p2\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Nortruthfulqa Mc Nno P2\",\n",
            "      \"description\": \"Nortruthfulqa Mc Nno P2 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_mc_nob_p0\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Nortruthfulqa Mc Nob P0\",\n",
            "      \"description\": \"Nortruthfulqa Mc Nob P0 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_mc_nob_p3\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Nortruthfulqa Mc Nob P3\",\n",
            "      \"description\": \"Nortruthfulqa Mc Nob P3 evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::tinyTruthfulQA\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Tinytruthfulqa\",\n",
            "      \"description\": \"Tinytruthfulqa evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_gen_ca\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Truthfulqa-Multi Gen Ca\",\n",
            "      \"description\": \"Truthfulqa-Multi Gen Ca evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_gen_eu\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Truthfulqa-Multi Gen Eu\",\n",
            "      \"description\": \"Truthfulqa-Multi Gen Eu evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"mc1\",\n",
            "        \"mc2\",\n",
            "        \"bleu\",\n",
            "        \"rouge\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_mc1_en\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Truthfulqa-Multi Mc1 En\",\n",
            "      \"description\": \"Truthfulqa-Multi Mc1 En evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_mc1_gl\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Truthfulqa-Multi Mc1 Gl\",\n",
            "      \"description\": \"Truthfulqa-Multi Mc1 Gl evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_mc2_es\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Truthfulqa-Multi Mc2 Es\",\n",
            "      \"description\": \"Truthfulqa-Multi Mc2 Es evaluation benchmark\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 817,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::bigbench_code_line_description_multiple_choice\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Bigbench Code Line Description Multiple Choice\",\n",
            "      \"description\": \"Bigbench Code Line Description Multiple Choice evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\",\n",
            "        \"acc_norm\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::ceval-valid_college_programming\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Ceval-Valid College Programming\",\n",
            "      \"description\": \"Ceval-Valid College Programming evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::code2text_javascript\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Code2Text Javascript\",\n",
            "      \"description\": \"Code2Text Javascript evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::code2text_ruby\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Code2Text Ruby\",\n",
            "      \"description\": \"Code2Text Ruby evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::humaneval\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Humaneval\",\n",
            "      \"description\": \"Humaneval evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::humaneval_instruct\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Humaneval Instruct\",\n",
            "      \"description\": \"Humaneval Instruct evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::mbpp_instruct\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Mbpp Instruct\",\n",
            "      \"description\": \"Mbpp Instruct evaluation benchmark\",\n",
            "      \"category\": \"code\",\n",
            "      \"metrics\": [\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"code\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"ragas::faithfulness\",\n",
            "      \"provider_id\": \"ragas\",\n",
            "      \"name\": \"Faithfulness\",\n",
            "      \"description\": \"Measures factual consistency of generated answer against given context\",\n",
            "      \"category\": \"rag_quality\",\n",
            "      \"metrics\": [\n",
            "        \"faithfulness_score\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": null,\n",
            "      \"tags\": [\n",
            "        \"rag\",\n",
            "        \"faithfulness\",\n",
            "        \"factuality\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"ragas::answer_relevancy\",\n",
            "      \"provider_id\": \"ragas\",\n",
            "      \"name\": \"Answer Relevancy\",\n",
            "      \"description\": \"Measures how relevant generated answer is to the question\",\n",
            "      \"category\": \"rag_quality\",\n",
            "      \"metrics\": [\n",
            "        \"answer_relevancy_score\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": null,\n",
            "      \"tags\": [\n",
            "        \"rag\",\n",
            "        \"relevancy\",\n",
            "        \"quality\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"ragas::context_precision\",\n",
            "      \"provider_id\": \"ragas\",\n",
            "      \"name\": \"Context Precision\",\n",
            "      \"description\": \"Measures precision of retrieved context\",\n",
            "      \"category\": \"rag_retrieval\",\n",
            "      \"metrics\": [\n",
            "        \"context_precision_score\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": null,\n",
            "      \"tags\": [\n",
            "        \"rag\",\n",
            "        \"retrieval\",\n",
            "        \"precision\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"ragas::context_recall\",\n",
            "      \"provider_id\": \"ragas\",\n",
            "      \"name\": \"Context Recall\",\n",
            "      \"description\": \"Measures recall of retrieved context\",\n",
            "      \"category\": \"rag_retrieval\",\n",
            "      \"metrics\": [\n",
            "        \"context_recall_score\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": null,\n",
            "      \"tags\": [\n",
            "        \"rag\",\n",
            "        \"retrieval\",\n",
            "        \"recall\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"garak::toxicity\",\n",
            "      \"provider_id\": \"garak\",\n",
            "      \"name\": \"Toxicity Detection\",\n",
            "      \"description\": \"Tests model's tendency to generate toxic content\",\n",
            "      \"category\": \"safety\",\n",
            "      \"metrics\": [\n",
            "        \"toxicity_rate\",\n",
            "        \"severity_score\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 500,\n",
            "      \"tags\": [\n",
            "        \"safety\",\n",
            "        \"toxicity\",\n",
            "        \"red_team\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"garak::bias_detection\",\n",
            "      \"provider_id\": \"garak\",\n",
            "      \"name\": \"Bias Detection\",\n",
            "      \"description\": \"Evaluates model for various forms of bias\",\n",
            "      \"category\": \"fairness\",\n",
            "      \"metrics\": [\n",
            "        \"bias_score\",\n",
            "        \"demographic_parity\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 1000,\n",
            "      \"tags\": [\n",
            "        \"fairness\",\n",
            "        \"bias\",\n",
            "        \"demographic\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"garak::pii_leakage\",\n",
            "      \"provider_id\": \"garak\",\n",
            "      \"name\": \"PII Leakage\",\n",
            "      \"description\": \"Tests for personally identifiable information leakage\",\n",
            "      \"category\": \"privacy\",\n",
            "      \"metrics\": [\n",
            "        \"pii_leak_rate\",\n",
            "        \"sensitivity_score\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 300,\n",
            "      \"tags\": [\n",
            "        \"privacy\",\n",
            "        \"pii\",\n",
            "        \"security\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"garak::prompt_injection\",\n",
            "      \"provider_id\": \"garak\",\n",
            "      \"name\": \"Prompt Injection\",\n",
            "      \"description\": \"Tests resilience against prompt injection attacks\",\n",
            "      \"category\": \"security\",\n",
            "      \"metrics\": [\n",
            "        \"injection_success_rate\",\n",
            "        \"defense_effectiveness\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 200,\n",
            "      \"tags\": [\n",
            "        \"security\",\n",
            "        \"injection\",\n",
            "        \"adversarial\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"total_count\": 176,\n",
            "  \"providers_included\": [\n",
            "    \"garak\",\n",
            "    \"lm_evaluation_harness\",\n",
            "    \"ragas\"\n",
            "  ]\n",
            "}\n",
            "--------------------------------------------------\n",
            "Total benchmarks available: 176\n",
            "  - ARC Easy (lm_evaluation_harness::arc_easy)\n",
            "    Category: reasoning\n",
            "    Provider: lm_evaluation_harness\n",
            "  - Aradice Boolq Lev (lm_evaluation_harness::AraDiCE_boolq_lev)\n",
            "    Category: general\n",
            "    Provider: lm_evaluation_harness\n",
            "  - Blimp (lm_evaluation_harness::blimp)\n",
            "    Category: general\n",
            "    Provider: lm_evaluation_harness\n",
            "  - Blimp Anaphor Gender Agreement (lm_evaluation_harness::blimp_anaphor_gender_agreement)\n",
            "    Category: general\n",
            "    Provider: lm_evaluation_harness\n",
            "  - Blimp Animate Subject Trans (lm_evaluation_harness::blimp_animate_subject_trans)\n",
            "    Category: general\n",
            "    Provider: lm_evaluation_harness\n"
          ]
        }
      ],
      "source": [
        "response = api_request(\"GET\", \"/benchmarks\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    benchmarks_data = response.json()\n",
        "    print(f\"Total benchmarks available: {benchmarks_data['total_count']}\")\n",
        "\n",
        "    # Show first 5 benchmarks\n",
        "    for benchmark in benchmarks_data['benchmarks'][:5]:\n",
        "        print(f\"  - {benchmark['name']} ({benchmark['benchmark_id']})\")\n",
        "        print(f\"    Category: {benchmark['category']}\")\n",
        "        print(f\"    Provider: {benchmark['provider_id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter Benchmarks by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/benchmarks\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"benchmarks\": [\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_stem_math_egy\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Aradice Arabicmmlu Primary Stem Math Egy\",\n",
            "      \"description\": \"Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_college_mathematics_light\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics\",\n",
            "      \"description\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_college_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Cmmlu College Mathematics\",\n",
            "      \"description\": \"Cmmlu College Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Cmmlu High School Mathematics\",\n",
            "      \"description\": \"Cmmlu High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Am High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Am High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Ar High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Ar High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_bn_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Bn High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Bn High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_cs_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Cs High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Cs High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_de_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full De High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full De High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_el_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full El High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full El High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_en_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full En High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full En High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_es_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Es High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Es High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_fa_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Fa High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Fa High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_fil_high_school_mathematics\",\n",
            "      \"provider_id\": \"lm_evaluation_harness\",\n",
            "      \"name\": \"Global Mmlu Full Fil High School Mathematics\",\n",
            "      \"description\": \"Global Mmlu Full Fil High School Mathematics evaluation benchmark\",\n",
            "      \"category\": \"math\",\n",
            "      \"metrics\": [\n",
            "        \"exact_match\",\n",
            "        \"accuracy\"\n",
            "      ],\n",
            "      \"num_few_shot\": 0,\n",
            "      \"dataset_size\": 14042,\n",
            "      \"tags\": [\n",
            "        \"math\",\n",
            "        \"lm_eval\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"total_count\": 15,\n",
            "  \"providers_included\": [\n",
            "    \"lm_evaluation_harness\"\n",
            "  ]\n",
            "}\n",
            "--------------------------------------------------\n",
            "Math benchmarks: 15\n",
            "  - Aradice Arabicmmlu Primary Stem Math Egy: Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\n",
            "  - Arabic Leaderboard Arabic Mmlu College Mathematics Light: Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\n",
            "  - Arabic Leaderboard Arabic Mmlu High School Mathematics: Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\n",
            "  - Cmmlu College Mathematics: Cmmlu College Mathematics evaluation benchmark\n",
            "  - Cmmlu High School Mathematics: Cmmlu High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full Am High School Mathematics: Global Mmlu Full Am High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full Ar High School Mathematics: Global Mmlu Full Ar High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full Bn High School Mathematics: Global Mmlu Full Bn High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full Cs High School Mathematics: Global Mmlu Full Cs High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full De High School Mathematics: Global Mmlu Full De High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full El High School Mathematics: Global Mmlu Full El High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full En High School Mathematics: Global Mmlu Full En High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full Es High School Mathematics: Global Mmlu Full Es High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full Fa High School Mathematics: Global Mmlu Full Fa High School Mathematics evaluation benchmark\n",
            "  - Global Mmlu Full Fil High School Mathematics: Global Mmlu Full Fil High School Mathematics evaluation benchmark\n"
          ]
        }
      ],
      "source": [
        "response = api_request(\"GET\", \"/benchmarks\", params={\"category\": \"math\"})\n",
        "\n",
        "if response.status_code == 200:\n",
        "    math_benchmarks = response.json()\n",
        "    print(f\"Math benchmarks: {math_benchmarks['total_count']}\")\n",
        "    for benchmark in math_benchmarks['benchmarks']:\n",
        "        print(f\"  - {benchmark['name']}: {benchmark['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Provider-Specific Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/providers/lm_evaluation_harness/benchmarks\n",
            "Status: 200\n",
            "Response:\n",
            "[\n",
            "  {\n",
            "    \"benchmark_id\": \"arc_easy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"ARC Easy\",\n",
            "    \"description\": \"ARC Easy evaluation benchmark - AI2 Reasoning Challenge (Easy)\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2376,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_boolq_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Boolq Lev\",\n",
            "    \"description\": \"Aradice Boolq Lev evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3270,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp\",\n",
            "    \"description\": \"Blimp evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_anaphor_gender_agreement\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Anaphor Gender Agreement\",\n",
            "    \"description\": \"Blimp Anaphor Gender Agreement evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_animate_subject_trans\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Animate Subject Trans\",\n",
            "    \"description\": \"Blimp Animate Subject Trans evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_coordinate_structure_constraint_complex_left_branch\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Coordinate Structure Constraint Complex Left Branch\",\n",
            "    \"description\": \"Blimp Coordinate Structure Constraint Complex Left Branch evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_determiner_noun_agreement_2\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Determiner Noun Agreement 2\",\n",
            "    \"description\": \"Blimp Determiner Noun Agreement 2 evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_determiner_noun_agreement_with_adj_2\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Determiner Noun Agreement With Adj 2\",\n",
            "    \"description\": \"Blimp Determiner Noun Agreement With Adj 2 evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_determiner_noun_agreement_with_adjective_1\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Determiner Noun Agreement With Adjective 1\",\n",
            "    \"description\": \"Blimp Determiner Noun Agreement With Adjective 1 evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_existential_there_object_raising\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Existential There Object Raising\",\n",
            "    \"description\": \"Blimp Existential There Object Raising evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_existential_there_subject_raising\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Existential There Subject Raising\",\n",
            "    \"description\": \"Blimp Existential There Subject Raising evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_intransitive\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Intransitive\",\n",
            "    \"description\": \"Blimp Intransitive evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_irregular_plural_subject_verb_agreement_1\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Irregular Plural Subject Verb Agreement 1\",\n",
            "    \"description\": \"Blimp Irregular Plural Subject Verb Agreement 1 evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_left_branch_island_simple_question\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Left Branch Island Simple Question\",\n",
            "    \"description\": \"Blimp Left Branch Island Simple Question evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_npi_present_2\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Npi Present 2\",\n",
            "    \"description\": \"Blimp Npi Present 2 evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_passive_1\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Passive 1\",\n",
            "    \"description\": \"Blimp Passive 1 evaluation benchmark\",\n",
            "    \"category\": \"general\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"general\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Egy evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_humanities_history_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu High Humanities History Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu High Humanities History Lev evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_humanities_philosophy_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu High Humanities Philosophy Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu High Humanities Philosophy Egy evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_language_arabic-language_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu High Language Arabic-Language Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu High Language Arabic-Language Lev evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Lev evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_language_arabic-language_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_humanities_islamic-studies_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_language_arabic-language-general_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_other_driving-test_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Na Other Driving-Test Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Na Other Driving-Test Egy evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_other_general-knowledge_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_language_arabic-language_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_other_management_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Univ Other Management Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Univ Other Management Egy evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_openbookqa_eng\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Openbookqa Eng\",\n",
            "    \"description\": \"Aradice Openbookqa Eng evaluation benchmark\",\n",
            "    \"category\": \"knowledge\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 500,\n",
            "    \"tags\": [\n",
            "      \"knowledge\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_boolq\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Boolq\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Boolq evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3270,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_boolq_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Boolq Light\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Boolq Light evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3270,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_mt_boolq_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Mt Boolq Light\",\n",
            "    \"description\": \"Arabic Mt Boolq Light evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3270,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"leaderboard_bbh_salient_translation_error_detection\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Leaderboard Bbh Salient Translation Error Detection\",\n",
            "    \"description\": \"Leaderboard Bbh Salient Translation Error Detection evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"bleu\",\n",
            "      \"chrf\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"aclue_ancient_chinese_culture\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aclue Ancient Chinese Culture\",\n",
            "    \"description\": \"Aclue Ancient Chinese Culture evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"african_flores\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"African Flores\",\n",
            "    \"description\": \"African Flores evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"bleu\",\n",
            "      \"chrf\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli-irokobench\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli-Irokobench\",\n",
            "    \"description\": \"Afrixnli-Irokobench evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli_amh_prompt_2\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli Amh Prompt 2\",\n",
            "    \"description\": \"Afrixnli Amh Prompt 2 evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli_amh_prompt_5\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli Amh Prompt 5\",\n",
            "    \"description\": \"Afrixnli Amh Prompt 5 evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli_en_direct_ewe\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli En Direct Ewe\",\n",
            "    \"description\": \"Afrixnli En Direct Ewe evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli_en_direct_ibo\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli En Direct Ibo\",\n",
            "    \"description\": \"Afrixnli En Direct Ibo evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli_en_direct_lug\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli En Direct Lug\",\n",
            "    \"description\": \"Afrixnli En Direct Lug evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli_en_direct_sot\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli En Direct Sot\",\n",
            "    \"description\": \"Afrixnli En Direct Sot evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli_en_direct_wol\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli En Direct Wol\",\n",
            "    \"description\": \"Afrixnli En Direct Wol evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"afrixnli_en_direct_zul\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Afrixnli En Direct Zul\",\n",
            "    \"description\": \"Afrixnli En Direct Zul evaluation benchmark\",\n",
            "    \"category\": \"multilingual\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 2000,\n",
            "    \"tags\": [\n",
            "      \"multilingual\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_stem_math_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Primary Stem Math Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_college_mathematics_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"cmmlu_college_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Cmmlu College Mathematics\",\n",
            "    \"description\": \"Cmmlu College Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"cmmlu_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Cmmlu High School Mathematics\",\n",
            "    \"description\": \"Cmmlu High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_am_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Am High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full Am High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_ar_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Ar High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full Ar High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_bn_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Bn High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full Bn High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_cs_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Cs High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full Cs High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_de_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full De High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full De High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_el_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full El High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full El High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_en_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full En High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full En High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_es_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Es High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full Es High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_fa_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Fa High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full Fa High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_fil_high_school_mathematics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Fil High School Mathematics\",\n",
            "    \"description\": \"Global Mmlu Full Fil High School Mathematics evaluation benchmark\",\n",
            "    \"category\": \"math\",\n",
            "    \"metrics\": [\n",
            "      \"exact_match\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"math\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_piqa_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Piqa Lev\",\n",
            "    \"description\": \"Aradice Piqa Lev evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1838,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_winogrande_eng\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Winogrande Eng\",\n",
            "    \"description\": \"Aradice Winogrande Eng evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1267,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_copa\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Copa\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Copa evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 500,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_copa_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Copa Light\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Copa Light evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 500,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_hellaswag\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 10042,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_hellaswag_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag Light\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag Light evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 10042,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_piqa\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Piqa\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Piqa evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1838,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_piqa_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Piqa Light\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Piqa Light evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1838,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_mt_hellaswag\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Mt Hellaswag\",\n",
            "    \"description\": \"Arabic Mt Hellaswag evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 10042,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_mt_piqa\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Mt Piqa\",\n",
            "    \"description\": \"Arabic Mt Piqa evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1838,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"copa_ar\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Copa Ar\",\n",
            "    \"description\": \"Copa Ar evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 500,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"copal_id_colloquial\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Copal Id Colloquial\",\n",
            "    \"description\": \"Copal Id Colloquial evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 500,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"darijahellaswag\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Darijahellaswag\",\n",
            "    \"description\": \"Darijahellaswag evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 10042,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"egyhellaswag\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Egyhellaswag\",\n",
            "    \"description\": \"Egyhellaswag evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 10042,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"hellaswag_ar\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Hellaswag Ar\",\n",
            "    \"description\": \"Hellaswag Ar evaluation benchmark\",\n",
            "    \"category\": \"reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 10042,\n",
            "    \"tags\": [\n",
            "      \"reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_race\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Race\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Race evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 674,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_race_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mt Race Light\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mt Race Light evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 674,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_mt_race_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Mt Race Light\",\n",
            "    \"description\": \"Arabic Mt Race Light evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 674,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"blimp_drop_argument\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Blimp Drop Argument\",\n",
            "    \"description\": \"Blimp Drop Argument evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 9536,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bigbench_gre_reading_comprehension_multiple_choice\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bigbench Gre Reading Comprehension Multiple Choice\",\n",
            "    \"description\": \"Bigbench Gre Reading Comprehension Multiple Choice evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3000,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"eus_reading\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Eus Reading\",\n",
            "    \"description\": \"Eus Reading evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3000,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"longbench_qasper\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Longbench Qasper\",\n",
            "    \"description\": \"Longbench Qasper evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3000,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"qasper_freeform\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Qasper Freeform\",\n",
            "    \"description\": \"Qasper Freeform evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3000,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"ruler_qa_squad\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Ruler Qa Squad\",\n",
            "    \"description\": \"Ruler Qa Squad evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3000,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"scrolls_qasper\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Scrolls Qasper\",\n",
            "    \"description\": \"Scrolls Qasper evaluation benchmark\",\n",
            "    \"category\": \"reading_comprehension\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 3000,\n",
            "    \"tags\": [\n",
            "      \"reading_comprehension\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_social-science_economics_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu High Social-Science Economics Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu High Social-Science Economics Egy evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_social-science_geography_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu High Social-Science Geography Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu High Social-Science Geography Lev evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_stem_computer-science_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu High Stem Computer-Science Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu High Stem Computer-Science Egy evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_stem_physics_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu High Stem Physics Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu High Stem Physics Lev evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_civics_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_economics_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_social-science_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_stem_computer-science_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_social-science_geography_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_social-science_social-science_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_stem_natural-science_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_social-science_accounting_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_social-science_political-science_egy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy\",\n",
            "    \"description\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_stem_computer-science_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev\",\n",
            "    \"description\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_college_biology_light\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mmlu College Biology Light\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mmlu College Biology Light evaluation benchmark\",\n",
            "    \"category\": \"science\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"science\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"agieval_logiqa_zh\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Agieval Logiqa Zh\",\n",
            "    \"description\": \"Agieval Logiqa Zh evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 651,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh\",\n",
            "    \"description\": \"Bbh evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot\",\n",
            "    \"description\": \"Bbh Cot Fewshot evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_causal_judgement\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Causal Judgement\",\n",
            "    \"description\": \"Bbh Cot Fewshot Causal Judgement evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_dyck_languages\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Dyck Languages\",\n",
            "    \"description\": \"Bbh Cot Fewshot Dyck Languages evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_hyperbaton\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Hyperbaton\",\n",
            "    \"description\": \"Bbh Cot Fewshot Hyperbaton evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_logical_deduction_three_objects\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Logical Deduction Three Objects\",\n",
            "    \"description\": \"Bbh Cot Fewshot Logical Deduction Three Objects evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_navigate\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Navigate\",\n",
            "    \"description\": \"Bbh Cot Fewshot Navigate evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_reasoning_about_colored_objects\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Reasoning About Colored Objects\",\n",
            "    \"description\": \"Bbh Cot Fewshot Reasoning About Colored Objects evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_snarks\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Snarks\",\n",
            "    \"description\": \"Bbh Cot Fewshot Snarks evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_tracking_shuffled_objects_five_objects\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects\",\n",
            "    \"description\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_fewshot_web_of_lies\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Fewshot Web Of Lies\",\n",
            "    \"description\": \"Bbh Cot Fewshot Web Of Lies evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 5,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_zeroshot\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Zeroshot\",\n",
            "    \"description\": \"Bbh Cot Zeroshot evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_zeroshot_causal_judgement\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Zeroshot Causal Judgement\",\n",
            "    \"description\": \"Bbh Cot Zeroshot Causal Judgement evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bbh_cot_zeroshot_dyck_languages\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bbh Cot Zeroshot Dyck Languages\",\n",
            "    \"description\": \"Bbh Cot Zeroshot Dyck Languages evaluation benchmark\",\n",
            "    \"category\": \"logic_reasoning\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"logic_reasoning\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_anatomy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mmlu Anatomy\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mmlu Anatomy evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_clinical_knowledge\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_medical_genetics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_professional_medicine\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine\",\n",
            "    \"description\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"cmmlu_professional_medicine\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Cmmlu Professional Medicine\",\n",
            "    \"description\": \"Cmmlu Professional Medicine evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"cmmlu_traditional_chinese_medicine\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Cmmlu Traditional Chinese Medicine\",\n",
            "    \"description\": \"Cmmlu Traditional Chinese Medicine evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_am_anatomy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Am Anatomy\",\n",
            "    \"description\": \"Global Mmlu Full Am Anatomy evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_am_clinical_knowledge\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Am Clinical Knowledge\",\n",
            "    \"description\": \"Global Mmlu Full Am Clinical Knowledge evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_am_medical_genetics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Am Medical Genetics\",\n",
            "    \"description\": \"Global Mmlu Full Am Medical Genetics evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_am_professional_medicine\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Am Professional Medicine\",\n",
            "    \"description\": \"Global Mmlu Full Am Professional Medicine evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_ar_anatomy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Ar Anatomy\",\n",
            "    \"description\": \"Global Mmlu Full Ar Anatomy evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_ar_clinical_knowledge\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Ar Clinical Knowledge\",\n",
            "    \"description\": \"Global Mmlu Full Ar Clinical Knowledge evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_ar_medical_genetics\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Ar Medical Genetics\",\n",
            "    \"description\": \"Global Mmlu Full Ar Medical Genetics evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_ar_professional_medicine\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Ar Professional Medicine\",\n",
            "    \"description\": \"Global Mmlu Full Ar Professional Medicine evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"global_mmlu_full_bn_anatomy\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Global Mmlu Full Bn Anatomy\",\n",
            "    \"description\": \"Global Mmlu Full Bn Anatomy evaluation benchmark\",\n",
            "    \"category\": \"medical\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 14042,\n",
            "    \"tags\": [\n",
            "      \"medical\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"lambada_openai\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Lambada Openai\",\n",
            "    \"description\": \"Lambada Openai evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"perplexity\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 5153,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"lambada_openai_mt_en\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Lambada Openai Mt En\",\n",
            "    \"description\": \"Lambada Openai Mt En evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"perplexity\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 5153,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"lambada_openai_mt_it\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Lambada Openai Mt It\",\n",
            "    \"description\": \"Lambada Openai Mt It evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"perplexity\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 5153,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"lambada_openai_mt_stablelm_es\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Lambada Openai Mt Stablelm Es\",\n",
            "    \"description\": \"Lambada Openai Mt Stablelm Es evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"perplexity\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 5153,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"lambada_openai_mt_stablelm_nl\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Lambada Openai Mt Stablelm Nl\",\n",
            "    \"description\": \"Lambada Openai Mt Stablelm Nl evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"perplexity\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 5153,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"lambada_standard_cloze_yaml\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Lambada Standard Cloze Yaml\",\n",
            "    \"description\": \"Lambada Standard Cloze Yaml evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"perplexity\",\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 5153,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"paloma_wikitext_103\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Paloma Wikitext 103\",\n",
            "    \"description\": \"Paloma Wikitext 103 evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 4358,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"pile_arxiv\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Pile Arxiv\",\n",
            "    \"description\": \"Pile Arxiv evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 210000000,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"pile_freelaw\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Pile Freelaw\",\n",
            "    \"description\": \"Pile Freelaw evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 210000000,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"pile_hackernews\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Pile Hackernews\",\n",
            "    \"description\": \"Pile Hackernews evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 210000000,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"pile_openwebtext2\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Pile Openwebtext2\",\n",
            "    \"description\": \"Pile Openwebtext2 evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 210000000,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"pile_ubuntu-irc\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Pile Ubuntu-Irc\",\n",
            "    \"description\": \"Pile Ubuntu-Irc evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 210000000,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"pile_youtubesubtitles\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Pile Youtubesubtitles\",\n",
            "    \"description\": \"Pile Youtubesubtitles evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 210000000,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"wikitext\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Wikitext\",\n",
            "    \"description\": \"Wikitext evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 4358,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"careqa_open_perplexity\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Careqa Open Perplexity\",\n",
            "    \"description\": \"Careqa Open Perplexity evaluation benchmark\",\n",
            "    \"category\": \"language_modeling\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 10000,\n",
            "    \"tags\": [\n",
            "      \"language_modeling\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"AraDiCE_truthfulqa_mc1_lev\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Aradice Truthfulqa Mc1 Lev\",\n",
            "    \"description\": \"Aradice Truthfulqa Mc1 Lev evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"metabench_truthfulqa_permute\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Metabench Truthfulqa Permute\",\n",
            "    \"description\": \"Metabench Truthfulqa Permute evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"nortruthfulqa_gen_nno_p0\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Nortruthfulqa Gen Nno P0\",\n",
            "    \"description\": \"Nortruthfulqa Gen Nno P0 evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"nortruthfulqa_gen_nno_p3\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Nortruthfulqa Gen Nno P3\",\n",
            "    \"description\": \"Nortruthfulqa Gen Nno P3 evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"nortruthfulqa_gen_nob_p1\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Nortruthfulqa Gen Nob P1\",\n",
            "    \"description\": \"Nortruthfulqa Gen Nob P1 evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"nortruthfulqa_gen_nob_p4\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Nortruthfulqa Gen Nob P4\",\n",
            "    \"description\": \"Nortruthfulqa Gen Nob P4 evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"nortruthfulqa_mc_nno_p2\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Nortruthfulqa Mc Nno P2\",\n",
            "    \"description\": \"Nortruthfulqa Mc Nno P2 evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"nortruthfulqa_mc_nob_p0\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Nortruthfulqa Mc Nob P0\",\n",
            "    \"description\": \"Nortruthfulqa Mc Nob P0 evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"nortruthfulqa_mc_nob_p3\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Nortruthfulqa Mc Nob P3\",\n",
            "    \"description\": \"Nortruthfulqa Mc Nob P3 evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"tinyTruthfulQA\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Tinytruthfulqa\",\n",
            "    \"description\": \"Tinytruthfulqa evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"truthfulqa-multi_gen_ca\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Truthfulqa-Multi Gen Ca\",\n",
            "    \"description\": \"Truthfulqa-Multi Gen Ca evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"truthfulqa-multi_gen_eu\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Truthfulqa-Multi Gen Eu\",\n",
            "    \"description\": \"Truthfulqa-Multi Gen Eu evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"mc1\",\n",
            "      \"mc2\",\n",
            "      \"bleu\",\n",
            "      \"rouge\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"truthfulqa-multi_mc1_en\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Truthfulqa-Multi Mc1 En\",\n",
            "    \"description\": \"Truthfulqa-Multi Mc1 En evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"truthfulqa-multi_mc1_gl\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Truthfulqa-Multi Mc1 Gl\",\n",
            "    \"description\": \"Truthfulqa-Multi Mc1 Gl evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"truthfulqa-multi_mc2_es\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Truthfulqa-Multi Mc2 Es\",\n",
            "    \"description\": \"Truthfulqa-Multi Mc2 Es evaluation benchmark\",\n",
            "    \"category\": \"safety\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 817,\n",
            "    \"tags\": [\n",
            "      \"safety\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"bigbench_code_line_description_multiple_choice\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Bigbench Code Line Description Multiple Choice\",\n",
            "    \"description\": \"Bigbench Code Line Description Multiple Choice evaluation benchmark\",\n",
            "    \"category\": \"code\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\",\n",
            "      \"acc_norm\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"code\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"ceval-valid_college_programming\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Ceval-Valid College Programming\",\n",
            "    \"description\": \"Ceval-Valid College Programming evaluation benchmark\",\n",
            "    \"category\": \"code\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"code\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"code2text_javascript\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Code2Text Javascript\",\n",
            "    \"description\": \"Code2Text Javascript evaluation benchmark\",\n",
            "    \"category\": \"code\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"code\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"code2text_ruby\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Code2Text Ruby\",\n",
            "    \"description\": \"Code2Text Ruby evaluation benchmark\",\n",
            "    \"category\": \"code\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"code\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"humaneval\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Humaneval\",\n",
            "    \"description\": \"Humaneval evaluation benchmark\",\n",
            "    \"category\": \"code\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"code\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"humaneval_instruct\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Humaneval Instruct\",\n",
            "    \"description\": \"Humaneval Instruct evaluation benchmark\",\n",
            "    \"category\": \"code\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"code\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  },\n",
            "  {\n",
            "    \"benchmark_id\": \"mbpp_instruct\",\n",
            "    \"provider_id\": \"lm_evaluation_harness\",\n",
            "    \"provider_name\": \"LM Evaluation Harness\",\n",
            "    \"name\": \"Mbpp Instruct\",\n",
            "    \"description\": \"Mbpp Instruct evaluation benchmark\",\n",
            "    \"category\": \"code\",\n",
            "    \"metrics\": [\n",
            "      \"accuracy\"\n",
            "    ],\n",
            "    \"num_few_shot\": 0,\n",
            "    \"dataset_size\": 1000,\n",
            "    \"tags\": [\n",
            "      \"code\",\n",
            "      \"lm_eval\"\n",
            "    ],\n",
            "    \"provider_type\": \"builtin\"\n",
            "  }\n",
            "]\n",
            "--------------------------------------------------\n",
            "Benchmarks for lm_evaluation_harness: 168\n",
            "\n",
            "Reasoning: 16 benchmarks\n",
            "  Examples: ARC Easy, Aradice Piqa Lev, Aradice Winogrande Eng\n",
            "\n",
            "General: 15 benchmarks\n",
            "  Examples: Aradice Boolq Lev, Blimp, Blimp Anaphor Gender Agreement\n",
            "\n",
            "Knowledge: 15 benchmarks\n",
            "  Examples: Aradice Arabicmmlu Egy, Aradice Arabicmmlu High Humanities History Lev, Aradice Arabicmmlu High Humanities Philosophy Egy\n",
            "\n",
            "Multilingual: 15 benchmarks\n",
            "  Examples: Arabic Leaderboard Arabic Mt Boolq, Arabic Leaderboard Arabic Mt Boolq Light, Arabic Mt Boolq Light\n",
            "\n",
            "Math: 15 benchmarks\n",
            "  Examples: Aradice Arabicmmlu Primary Stem Math Egy, Arabic Leaderboard Arabic Mmlu College Mathematics Light, Arabic Leaderboard Arabic Mmlu High School Mathematics\n",
            "\n",
            "Reading_Comprehension: 10 benchmarks\n",
            "  Examples: Arabic Leaderboard Arabic Mt Race, Arabic Leaderboard Arabic Mt Race Light, Arabic Mt Race Light\n",
            "\n",
            "Science: 15 benchmarks\n",
            "  Examples: Aradice Arabicmmlu High Social-Science Economics Egy, Aradice Arabicmmlu High Social-Science Geography Lev, Aradice Arabicmmlu High Stem Computer-Science Egy\n",
            "\n",
            "Logic_Reasoning: 15 benchmarks\n",
            "  Examples: Agieval Logiqa Zh, Bbh, Bbh Cot Fewshot\n",
            "\n",
            "Medical: 15 benchmarks\n",
            "  Examples: Arabic Leaderboard Arabic Mmlu Anatomy, Arabic Leaderboard Arabic Mmlu Clinical Knowledge, Arabic Leaderboard Arabic Mmlu Medical Genetics\n",
            "\n",
            "Language_Modeling: 15 benchmarks\n",
            "  Examples: Lambada Openai, Lambada Openai Mt En, Lambada Openai Mt It\n",
            "\n",
            "Safety: 15 benchmarks\n",
            "  Examples: Aradice Truthfulqa Mc1 Lev, Metabench Truthfulqa Permute, Nortruthfulqa Gen Nno P0\n",
            "\n",
            "Code: 7 benchmarks\n",
            "  Examples: Bigbench Code Line Description Multiple Choice, Ceval-Valid College Programming, Code2Text Javascript\n"
          ]
        }
      ],
      "source": [
        "provider_id = \"lm_evaluation_harness\"\n",
        "response = api_request(\"GET\", f\"/providers/{provider_id}/benchmarks\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    benchmarks = response.json()\n",
        "    print(f\"Benchmarks for {provider_id}: {len(benchmarks)}\")\n",
        "\n",
        "    # Group by category\n",
        "    categories = {}\n",
        "    for benchmark in benchmarks:\n",
        "        category = benchmark['category']\n",
        "        if category not in categories:\n",
        "            categories[category] = []\n",
        "        categories[category].append(benchmark['name'])\n",
        "\n",
        "    for category, names in categories.items():\n",
        "        print(f\"\\n{category.title()}: {len(names)} benchmarks\")\n",
        "        print(f\"  Examples: {', '.join(names[:3])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collections\n",
        "\n",
        "### List Available Collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/collections\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"collections\": [\n",
            "    {\n",
            "      \"collection_id\": \"healthcare_safety_v1\",\n",
            "      \"name\": \"Healthcare Safety Collection v1\",\n",
            "      \"description\": \"Comprehensive healthcare AI safety evaluation suite\",\n",
            "      \"benchmarks\": [\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"truthfulqa\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"pubmedqa\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"medmcqa\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"garak\",\n",
            "          \"benchmark_id\": \"bias_detection\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"garak\",\n",
            "          \"benchmark_id\": \"pii_leakage\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"collection_id\": \"automotive_safety_v1\",\n",
            "      \"name\": \"Automotive Safety Collection v1\",\n",
            "      \"description\": \"Automotive AI safety and reliability evaluation suite\",\n",
            "      \"benchmarks\": [\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"hellaswag\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"arc_challenge\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"commonsense_qa\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"garak\",\n",
            "          \"benchmark_id\": \"toxicity\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"collection_id\": \"finance_compliance_v1\",\n",
            "      \"name\": \"Financial Compliance Collection v1\",\n",
            "      \"description\": \"Financial AI compliance and accuracy evaluation suite\",\n",
            "      \"benchmarks\": [\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"gsm8k\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"mathqa\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"truthfulqa\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"garak\",\n",
            "          \"benchmark_id\": \"pii_leakage\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"collection_id\": \"general_llm_eval_v1\",\n",
            "      \"name\": \"General LLM Evaluation v1\",\n",
            "      \"description\": \"Comprehensive general-purpose LLM evaluation suite\",\n",
            "      \"benchmarks\": [\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"mmlu\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"hellaswag\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"arc_challenge\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"truthfulqa\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"gsm8k\"\n",
            "        },\n",
            "        {\n",
            "          \"provider_id\": \"lm_evaluation_harness\",\n",
            "          \"benchmark_id\": \"winogrande\"\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"total_collections\": 4\n",
            "}\n",
            "--------------------------------------------------\n",
            "Available collections: 4\n",
            "\n",
            "📁 Healthcare Safety Collection v1 (healthcare_safety_v1)\n",
            "   Description: Comprehensive healthcare AI safety evaluation suite\n",
            "   Benchmarks: 5\n",
            "     - lm_evaluation_harness::truthfulqa\n",
            "     - lm_evaluation_harness::pubmedqa\n",
            "     - lm_evaluation_harness::medmcqa\n",
            "\n",
            "📁 Automotive Safety Collection v1 (automotive_safety_v1)\n",
            "   Description: Automotive AI safety and reliability evaluation suite\n",
            "   Benchmarks: 4\n",
            "     - lm_evaluation_harness::hellaswag\n",
            "     - lm_evaluation_harness::arc_challenge\n",
            "     - lm_evaluation_harness::commonsense_qa\n",
            "\n",
            "📁 Financial Compliance Collection v1 (finance_compliance_v1)\n",
            "   Description: Financial AI compliance and accuracy evaluation suite\n",
            "   Benchmarks: 4\n",
            "     - lm_evaluation_harness::gsm8k\n",
            "     - lm_evaluation_harness::mathqa\n",
            "     - lm_evaluation_harness::truthfulqa\n",
            "\n",
            "📁 General LLM Evaluation v1 (general_llm_eval_v1)\n",
            "   Description: Comprehensive general-purpose LLM evaluation suite\n",
            "   Benchmarks: 6\n",
            "     - lm_evaluation_harness::mmlu\n",
            "     - lm_evaluation_harness::hellaswag\n",
            "     - lm_evaluation_harness::arc_challenge\n"
          ]
        }
      ],
      "source": [
        "response = api_request(\"GET\", \"/collections\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    collections = response.json()\n",
        "    print(f\"Available collections: {collections['total_collections']}\")\n",
        "\n",
        "    for collection in collections['collections']:\n",
        "        print(f\"\\n📁 {collection['name']} ({collection['collection_id']})\")\n",
        "        print(f\"   Description: {collection['description']}\")\n",
        "        print(f\"   Benchmarks: {len(collection['benchmarks'])}\")\n",
        "        for benchmark_ref in collection['benchmarks'][:3]:  # Show first 3\n",
        "            print(f\"     - {benchmark_ref['provider_id']}::{benchmark_ref['benchmark_id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Server Management\n",
        "\n",
        "### List All Model Servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/servers\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"servers\": [\n",
            "    {\n",
            "      \"server_id\": \"vllm\",\n",
            "      \"server_type\": \"vllm\",\n",
            "      \"base_url\": \"http://vllm-server.test.svc.cluster.local:8000\",\n",
            "      \"model_count\": 1,\n",
            "      \"status\": \"active\",\n",
            "      \"tags\": [\n",
            "        \"runtime\"\n",
            "      ],\n",
            "      \"created_at\": \"2025-11-08T23:18:51.920032\"\n",
            "    }\n",
            "  ],\n",
            "  \"total_servers\": 1,\n",
            "  \"runtime_servers\": [\n",
            "    {\n",
            "      \"server_id\": \"vllm\",\n",
            "      \"server_type\": \"vllm\",\n",
            "      \"base_url\": \"http://vllm-server.test.svc.cluster.local:8000\",\n",
            "      \"model_count\": 1,\n",
            "      \"status\": \"active\",\n",
            "      \"tags\": [\n",
            "        \"runtime\"\n",
            "      ],\n",
            "      \"created_at\": \"2025-11-08T23:18:51.920032\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "--------------------------------------------------\n",
            "Total servers: 1\n",
            "Runtime servers: 1\n",
            "\n",
            "📋 Model Servers:\n",
            "  - vllm\n",
            "    Type: vllm\n",
            "    Base URL: http://vllm-server.test.svc.cluster.local:8000\n",
            "    Models: 1\n",
            "    Status: active\n"
          ]
        }
      ],
      "source": [
        "response = api_request(\"GET\", \"/servers\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    servers_data = response.json()\n",
        "    print(f\"Total servers: {servers_data['total_servers']}\")\n",
        "    print(f\"Runtime servers: {len(servers_data.get('runtime_servers', []))}\")\n",
        "    \n",
        "    print(\"\\n📋 Model Servers:\")\n",
        "    for server in servers_data.get('servers', []):\n",
        "        print(f\"  - {server['server_id']}\")\n",
        "        print(f\"    Type: {server['server_type']}\")\n",
        "        print(f\"    Base URL: {server['base_url']}\")\n",
        "        print(f\"    Models: {server['model_count']}\")\n",
        "        print(f\"    Status: {server['status']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Only Active Servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/servers\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"servers\": [\n",
            "    {\n",
            "      \"server_id\": \"vllm\",\n",
            "      \"server_type\": \"vllm\",\n",
            "      \"base_url\": \"http://vllm-server.test.svc.cluster.local:8000\",\n",
            "      \"model_count\": 1,\n",
            "      \"status\": \"active\",\n",
            "      \"tags\": [\n",
            "        \"runtime\"\n",
            "      ],\n",
            "      \"created_at\": \"2025-11-08T23:18:54.380045\"\n",
            "    }\n",
            "  ],\n",
            "  \"total_servers\": 1,\n",
            "  \"runtime_servers\": [\n",
            "    {\n",
            "      \"server_id\": \"vllm\",\n",
            "      \"server_type\": \"vllm\",\n",
            "      \"base_url\": \"http://vllm-server.test.svc.cluster.local:8000\",\n",
            "      \"model_count\": 1,\n",
            "      \"status\": \"active\",\n",
            "      \"tags\": [\n",
            "        \"runtime\"\n",
            "      ],\n",
            "      \"created_at\": \"2025-11-08T23:18:54.380045\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "--------------------------------------------------\n",
            "Active servers: 1\n",
            "  - vllm - active\n"
          ]
        }
      ],
      "source": [
        "response = api_request(\"GET\", \"/servers\", params={\"include_inactive\": False})\n",
        "\n",
        "if response.status_code == 200:\n",
        "    servers_data = response.json()\n",
        "    print(f\"Active servers: {servers_data['total_servers']}\")\n",
        "    for server in servers_data.get('servers', []):\n",
        "        print(f\"  - {server['server_id']} - {server['status']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Server by ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/servers/vllm\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"server_id\": \"vllm\",\n",
            "  \"server_type\": \"vllm\",\n",
            "  \"base_url\": \"http://vllm-server.test.svc.cluster.local:8000\",\n",
            "  \"api_key_required\": true,\n",
            "  \"models\": [\n",
            "    {\n",
            "      \"model_name\": \"vllm\",\n",
            "      \"description\": null,\n",
            "      \"capabilities\": null,\n",
            "      \"config\": null,\n",
            "      \"status\": \"active\",\n",
            "      \"tags\": [\n",
            "        \"runtime\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"server_config\": null,\n",
            "  \"status\": \"active\",\n",
            "  \"tags\": [\n",
            "    \"runtime\"\n",
            "  ],\n",
            "  \"created_at\": \"2025-11-08T23:18:56.893617\",\n",
            "  \"updated_at\": \"2025-11-08T23:18:56.893618\"\n",
            "}\n",
            "--------------------------------------------------\n",
            "Server ID: vllm\n",
            "Type: vllm\n",
            "Base URL: http://vllm-server.test.svc.cluster.local:8000\n",
            "Status: active\n",
            "\n",
            "📦 Models on this server (1):\n",
            "  - vllm\n",
            "    Status: active\n",
            "\n",
            "Tags: runtime\n"
          ]
        }
      ],
      "source": [
        "# Get details for a specific model server\n",
        "server_id = \"vllm\"  # Replace with an actual server ID from your system\n",
        "response = api_request(\"GET\", f\"/servers/{server_id}\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    server = response.json()\n",
        "    print(f\"Server ID: {server['server_id']}\")\n",
        "    print(f\"Type: {server['server_type']}\")\n",
        "    print(f\"Base URL: {server['base_url']}\")\n",
        "    print(f\"Status: {server['status']}\")\n",
        "    \n",
        "    print(f\"\\n📦 Models on this server ({len(server['models'])}):\")\n",
        "    for model in server['models']:\n",
        "        print(f\"  - {model['model_name']}\")\n",
        "        print(f\"    Status: {model['status']}\")\n",
        "        if model.get('description'):\n",
        "            print(f\"    Description: {model['description']}\")\n",
        "    \n",
        "    if server.get('tags'):\n",
        "        print(f\"\\nTags: {', '.join(server['tags'])}\")\n",
        "elif response.status_code == 404:\n",
        "    print(f\"❌ Server '{server_id}' not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Model by Server and Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET http://localhost:8000/api/v1/servers/vllm\n",
            "Status: 200\n",
            "Response:\n",
            "{\n",
            "  \"server_id\": \"vllm\",\n",
            "  \"server_type\": \"vllm\",\n",
            "  \"base_url\": \"http://vllm-server.test.svc.cluster.local:8000\",\n",
            "  \"api_key_required\": true,\n",
            "  \"models\": [\n",
            "    {\n",
            "      \"model_name\": \"vllm\",\n",
            "      \"description\": null,\n",
            "      \"capabilities\": null,\n",
            "      \"config\": null,\n",
            "      \"status\": \"active\",\n",
            "      \"tags\": [\n",
            "        \"runtime\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"server_config\": null,\n",
            "  \"status\": \"active\",\n",
            "  \"tags\": [\n",
            "    \"runtime\"\n",
            "  ],\n",
            "  \"created_at\": \"2025-11-08T23:18:59.483593\",\n",
            "  \"updated_at\": \"2025-11-08T23:18:59.483594\"\n",
            "}\n",
            "--------------------------------------------------\n",
            "❌ Model 'tinyllama' not found on server 'vllm'\n"
          ]
        }
      ],
      "source": [
        "# Get a specific model by getting the server and finding the model in its models list\n",
        "server_id = \"vllm\"\n",
        "model_name = \"tinyllama\"  # Replace with actual model name\n",
        "\n",
        "response = api_request(\"GET\", f\"/servers/{server_id}\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    server = response.json()\n",
        "    model = None\n",
        "    for m in server['models']:\n",
        "        if m['model_name'] == model_name:\n",
        "            model = m\n",
        "            break\n",
        "    \n",
        "    if model:\n",
        "        print(f\"✅ Found model: {model['model_name']}\")\n",
        "        print(f\"   Server: {server['server_id']}\")\n",
        "        print(f\"   Status: {model['status']}\")\n",
        "        if model.get('description'):\n",
        "            print(f\"   Description: {model['description']}\")\n",
        "        if model.get('capabilities'):\n",
        "            print(f\"   Capabilities: {model['capabilities']}\")\n",
        "    else:\n",
        "        print(f\"❌ Model '{model_name}' not found on server '{server_id}'\")\n",
        "else:\n",
        "    print(f\"❌ Server '{server_id}' not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register a New Model Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register a model server with models\n",
        "new_server = {\n",
        "    \"server_id\": \"groq-server\",\n",
        "    \"server_type\": \"openai-compatible\",\n",
        "    \"base_url\": \"https://api.groq.com/openai/v1\",\n",
        "    \"api_key_required\": True,\n",
        "    \"models\": [\n",
        "        {\n",
        "            \"model_name\": \"llama-3.1-70b\",\n",
        "            \"description\": \"Meta's Llama 3.1 70B model\",\n",
        "            \"status\": \"active\",\n",
        "            \"tags\": [\"groq\", \"llama\", \"70b\"]\n",
        "        },\n",
        "        {\n",
        "            \"model_name\": \"llama-3.1-8b\",\n",
        "            \"description\": \"Meta's Llama 3.1 8B model\",\n",
        "            \"status\": \"active\",\n",
        "            \"tags\": [\"groq\", \"llama\", \"8b\"]\n",
        "        }\n",
        "    ],\n",
        "    \"server_config\": {\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"timeout\": 60,\n",
        "        \"retry_attempts\": 3\n",
        "    },\n",
        "    \"status\": \"active\",\n",
        "    \"tags\": [\"groq\", \"openai-compatible\", \"fast\"]\n",
        "}\n",
        "\n",
        "print(\"📝 Registering new model server...\")\n",
        "print_json(new_server)\n",
        "\n",
        "response = api_request(\"POST\", \"/servers\", json=new_server)\n",
        "\n",
        "if response.status_code == 201:\n",
        "    registered_server = response.json()\n",
        "    print(f\"✅ Model server registered successfully!\")\n",
        "    print(f\"Server ID: {registered_server['server_id']}\")\n",
        "    print(f\"Models: {len(registered_server['models'])}\")\n",
        "    print(f\"Created at: {registered_server.get('created_at', 'N/A')}\")\n",
        "else:\n",
        "    print(f\"❌ Failed to register server: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register a vLLM Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register a vLLM server\n",
        "vllm_server = {\n",
        "    \"server_id\": \"local-vllm\",\n",
        "    \"server_type\": \"vllm\",\n",
        "    \"base_url\": \"http://localhost:8000\",\n",
        "    \"api_key_required\": False,\n",
        "    \"models\": [\n",
        "        {\n",
        "            \"model_name\": \"llama-2-7b\",\n",
        "            \"description\": \"Llama 2 7B running on local vLLM server\",\n",
        "            \"status\": \"active\",\n",
        "            \"tags\": [\"vllm\", \"local\", \"llama-2\"]\n",
        "        }\n",
        "    ],\n",
        "    \"status\": \"active\",\n",
        "    \"tags\": [\"vllm\", \"local\"]\n",
        "}\n",
        "\n",
        "print(\"📝 Registering vLLM server...\")\n",
        "response = api_request(\"POST\", \"/servers\", json=vllm_server)\n",
        "\n",
        "if response.status_code == 201:\n",
        "    print(f\"✅ vLLM server registered: {response.json()['server_id']}\")\n",
        "else:\n",
        "    print(f\"⚠️ Note: This may fail if the server ID already exists\")\n",
        "    print(f\"Response: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Update a Model Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update server details\n",
        "server_id = \"groq-server\"  # Replace with an actual server ID\n",
        "\n",
        "update_request = {\n",
        "    \"status\": \"active\",\n",
        "    \"tags\": [\"groq\", \"openai-compatible\", \"fast\", \"updated\"]\n",
        "}\n",
        "\n",
        "print(f\"📝 Updating server: {server_id}\")\n",
        "print_json(update_request)\n",
        "\n",
        "response = api_request(\"PUT\", f\"/servers/{server_id}\", json=update_request)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    updated_server = response.json()\n",
        "    print(f\"✅ Server updated successfully!\")\n",
        "    print(f\"Server ID: {updated_server['server_id']}\")\n",
        "    print(f\"Tags: {', '.join(updated_server.get('tags', []))}\")\n",
        "elif response.status_code == 404:\n",
        "    print(f\"❌ Server '{server_id}' not found\")\n",
        "else:\n",
        "    print(f\"❌ Failed to update server: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete a Model Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete a model server (runtime servers cannot be deleted via API)\n",
        "server_id = \"groq-server\"  # Replace with an actual server ID\n",
        "\n",
        "print(f\"🗑️ Deleting server: {server_id}\")\n",
        "response = api_request(\"DELETE\", f\"/servers/{server_id}\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(f\"✅ {result.get('message', 'Server deleted successfully')}\")\n",
        "elif response.status_code == 404:\n",
        "    print(f\"❌ Server '{server_id}' not found\")\n",
        "elif response.status_code == 400:\n",
        "    print(f\"❌ Cannot delete runtime server (configured via environment variables)\")\n",
        "    print(f\"Response: {response.text}\")\n",
        "else:\n",
        "    print(f\"❌ Failed to delete server: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reload Runtime Servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload model servers configured via environment variables\n",
        "print(\"🔄 Reloading runtime servers from environment variables...\")\n",
        "response = api_request(\"POST\", \"/servers/reload\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(f\"✅ {result.get('message', 'Runtime servers reloaded successfully')}\")\n",
        "    \n",
        "    # List servers again to see any new runtime servers\n",
        "    print(\"\\n📋 Updated server list:\")\n",
        "    list_response = api_request(\"GET\", \"/servers\")\n",
        "    if list_response.status_code == 200:\n",
        "        servers_data = list_response.json()\n",
        "        print(f\"Total servers: {servers_data['total_servers']}\")\n",
        "        print(f\"Runtime servers: {len(servers_data.get('runtime_servers', []))}\")\n",
        "else:\n",
        "    print(f\"❌ Failed to reload servers: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Evaluation Examples\n",
        "\n",
        "### Single Benchmark Evaluation from Builtin Provider (Simplified API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Creating single benchmark evaluation request...\n",
            "Provider ID: lm_evaluation_harness\n",
            "Benchmark ID: arc_easy\n",
            "{\n",
            "  \"model\": {\n",
            "    \"server\": \"vllm\",\n",
            "    \"name\": \"tinyllama\"\n",
            "  },\n",
            "  \"model_configuration\": {\n",
            "    \"temperature\": 0.0,\n",
            "    \"max_tokens\": 512\n",
            "  },\n",
            "  \"timeout_minutes\": 30,\n",
            "  \"retry_attempts\": 1,\n",
            "  \"limit\": 100,\n",
            "  \"num_fewshot\": 0,\n",
            "  \"experiment_name\": \"Single Benchmark - ARC Easy\",\n",
            "  \"tags\": {\n",
            "    \"example_type\": \"single_benchmark\",\n",
            "    \"provider\": \"lm_evaluation_harness\",\n",
            "    \"benchmark\": \"arc_easy\"\n",
            "  }\n",
            "}\n",
            "POST http://localhost:8000/api/v1/evaluations/benchmarks/lm_evaluation_harness/arc_easy\n",
            "Status: 202\n",
            "Response:\n",
            "{\n",
            "  \"request_id\": \"a24b471e-edcb-4130-8b74-07219d25e108\",\n",
            "  \"status\": \"pending\",\n",
            "  \"total_evaluations\": 0,\n",
            "  \"completed_evaluations\": 0,\n",
            "  \"failed_evaluations\": 0,\n",
            "  \"results\": [],\n",
            "  \"aggregated_metrics\": {},\n",
            "  \"experiment_url\": \"http://localhost:5000/#/experiments/exp_c6a74a5e\",\n",
            "  \"created_at\": \"2025-11-08T23:19:07.245302Z\",\n",
            "  \"updated_at\": \"2025-11-08T23:19:07.245493\",\n",
            "  \"estimated_completion\": null,\n",
            "  \"progress_percentage\": 0.0\n",
            "}\n",
            "--------------------------------------------------\n",
            "✅ Single benchmark evaluation created successfully!\n",
            "Request ID: a24b471e-edcb-4130-8b74-07219d25e108\n",
            "Status: pending\n",
            "Experiment URL: http://localhost:5000/#/experiments/exp_c6a74a5e\n"
          ]
        }
      ],
      "source": [
        "# Example: Run a single benchmark using the simplified API (Llama Stack compatible)\n",
        "provider_id = \"lm_evaluation_harness\"\n",
        "benchmark_id = \"arc_easy\"\n",
        "\n",
        "single_benchmark_request = {\n",
        "    \"model\": {\n",
        "        \"server\": \"vllm\",\n",
        "        \"name\": \"tinyllama\"\n",
        "    },\n",
        "    \"model_configuration\": {\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 512\n",
        "    },\n",
        "    \"timeout_minutes\": 30,\n",
        "    \"retry_attempts\": 1,\n",
        "    \"limit\": 100,  # Limit to 100 samples for faster execution\n",
        "    \"num_fewshot\": 0,\n",
        "    \"experiment_name\": \"Single Benchmark - ARC Easy\",\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"single_benchmark\",\n",
        "        \"provider\": \"lm_evaluation_harness\",\n",
        "        \"benchmark\": \"arc_easy\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"📝 Creating single benchmark evaluation request...\")\n",
        "print(f\"Provider ID: {provider_id}\")\n",
        "print(f\"Benchmark ID: {benchmark_id}\")\n",
        "print_json(single_benchmark_request)\n",
        "\n",
        "response = api_request(\"POST\", f\"/evaluations/benchmarks/{provider_id}/{benchmark_id}\", json=single_benchmark_request)\n",
        "\n",
        "if response.status_code == 202:\n",
        "    evaluation_response = response.json()\n",
        "    request_id = evaluation_response[\"request_id\"]\n",
        "    print(f\"✅ Single benchmark evaluation created successfully!\")\n",
        "    print(f\"Request ID: {request_id}\")\n",
        "    print(f\"Status: {evaluation_response['status']}\")\n",
        "    print(f\"Experiment URL: {evaluation_response.get('experiment_url', 'N/A')}\")\n",
        "else:\n",
        "    print(\"❌ Failed to create evaluation\")\n",
        "    print(f\"Error: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple Evaluation with Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple evaluation request using risk category\n",
        "evaluation_request = {\n",
        "    \"request_id\": str(uuid4()),\n",
        "    \"experiment_name\": \"Simple Risk-Based Evaluation\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"GPT-4 Mini Low Risk Evaluation\",\n",
        "            \"description\": \"Basic evaluation using low risk benchmarks\",\n",
        "            \"model\": {\n",
        "                \"server\": \"default\",\n",
        "                \"name\": \"default\"\n",
        "            },\n",
        "            \"model_configuration\": {\n",
        "                \"temperature\": 0.0,\n",
        "                \"max_tokens\": 512\n",
        "            },\n",
        "            \"risk_category\": \"low\",\n",
        "            \"timeout_minutes\": 30,\n",
        "            \"retry_attempts\": 1\n",
        "        }\n",
        "    ],\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"risk_category\",\n",
        "        \"complexity\": \"simple\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"📝 Creating evaluation request...\")\n",
        "print_json(evaluation_request)\n",
        "\n",
        "response = api_request(\"POST\", \"/evaluations\", json=evaluation_request)\n",
        "\n",
        "if response.status_code == 202:\n",
        "    evaluation_response = response.json()\n",
        "    request_id = evaluation_response[\"request_id\"]\n",
        "    print(f\"✅ Evaluation created successfully!\")\n",
        "    print(f\"Request ID: {request_id}\")\n",
        "    print(f\"Status: {evaluation_response['status']}\")\n",
        "    print(f\"Experiment URL: {evaluation_response.get('experiment_url', 'N/A')}\")\n",
        "else:\n",
        "    print(\"❌ Failed to create evaluation\")\n",
        "    print(f\"Error: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation with Explicit Backend Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an evaluation with explicit backend configuration\n",
        "explicit_evaluation = {\n",
        "    \"request_id\": str(uuid4()),\n",
        "    \"experiment_name\": \"Explicit Backend Configuration\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"LM-Eval Harness Evaluation\",\n",
        "            \"description\": \"Evaluation with explicit lm-evaluation-harness configuration\",\n",
        "            \"model\": {\n",
        "                \"server\": \"default\",\n",
        "                \"name\": \"default\"\n",
        "            },\n",
        "            \"model_configuration\": {\n",
        "                \"temperature\": 0.1,\n",
        "                \"max_tokens\": 256,\n",
        "                \"top_p\": 0.95\n",
        "            },\n",
        "            \"backends\": [\n",
        "                {\n",
        "                    \"name\": \"lm-eval-backend\",\n",
        "                    \"type\": \"lm-evaluation-harness\",\n",
        "                    \"config\": {\n",
        "                        \"batch_size\": 1,\n",
        "                        \"device\": \"cpu\"\n",
        "                    },\n",
        "                    \"benchmarks\": [\n",
        "                        {\n",
        "                            \"name\": \"arc_easy\",\n",
        "                            \"tasks\": [\"arc_easy\"],\n",
        "                            \"config\": {\n",
        "                                \"num_fewshot\": 5,\n",
        "                                \"limit\": 50\n",
        "                            }\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"hellaswag\",\n",
        "                            \"tasks\": [\"hellaswag\"],\n",
        "                            \"config\": {\n",
        "                                \"num_fewshot\": 10,\n",
        "                                \"limit\": 100\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            \"timeout_minutes\": 45,\n",
        "            \"retry_attempts\": 2\n",
        "        }\n",
        "    ],\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"explicit_backend\",\n",
        "        \"complexity\": \"intermediate\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"📝 Creating evaluation with explicit backend...\")\n",
        "response = api_request(\"POST\", \"/evaluations\", json=explicit_evaluation)\n",
        "\n",
        "if response.status_code == 202:\n",
        "    explicit_response = response.json()\n",
        "    explicit_request_id = explicit_response[\"request_id\"]\n",
        "    print(f\"✅ Explicit evaluation created!\")\n",
        "    print(f\"Request ID: {explicit_request_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NeMo Evaluator Integration\n",
        "\n",
        "### Single NeMo Evaluator Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with single NeMo Evaluator container\n",
        "nemo_single_evaluation = {\n",
        "    \"request_id\": str(uuid4()),\n",
        "    \"experiment_name\": \"NeMo Evaluator Single Container\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"GPT-4 via NeMo Evaluator\",\n",
        "            \"description\": \"Remote evaluation using NeMo Evaluator container\",\n",
        "            \"model\": {\n",
        "                \"server\": \"default\",\n",
        "                \"name\": \"default\"\n",
        "            },\n",
        "            \"model_configuration\": {\n",
        "                \"temperature\": 0.0,\n",
        "                \"max_tokens\": 512,\n",
        "                \"top_p\": 0.95\n",
        "            },\n",
        "            \"backends\": [\n",
        "                {\n",
        "                    \"name\": \"remote-nemo-evaluator\",\n",
        "                    \"type\": \"nemo-evaluator\",\n",
        "                    \"config\": {\n",
        "                        \"endpoint\": \"localhost\",\n",
        "                        \"port\": 3825,\n",
        "                        \"model_endpoint\": \"https://api.openai.com/v1/chat/completions\",\n",
        "                        \"endpoint_type\": \"chat\",\n",
        "                        \"api_key_env\": \"OPENAI_API_KEY\",\n",
        "                        \"timeout_seconds\": 1800,\n",
        "                        \"max_retries\": 2,\n",
        "                        \"verify_ssl\": False,\n",
        "                        \"framework_name\": \"eval-hub-example\",\n",
        "                        \"parallelism\": 1,\n",
        "                        \"limit_samples\": 25,\n",
        "                        \"temperature\": 0.0,\n",
        "                        \"top_p\": 0.95\n",
        "                    },\n",
        "                    \"benchmarks\": [\n",
        "                        {\n",
        "                            \"name\": \"mmlu_pro_sample\",\n",
        "                            \"tasks\": [\"mmlu_pro\"],\n",
        "                            \"config\": {\n",
        "                                \"limit\": 25,\n",
        "                                \"num_fewshot\": 5\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            \"timeout_minutes\": 60,\n",
        "            \"retry_attempts\": 1\n",
        "        }\n",
        "    ],\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"nemo_evaluator_single\",\n",
        "        \"complexity\": \"advanced\",\n",
        "        \"backend\": \"remote_container\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"📝 Creating NeMo Evaluator evaluation...\")\n",
        "print(\"Note: This requires a running NeMo Evaluator container on localhost:3825\")\n",
        "\n",
        "response = api_request(\"POST\", \"/evaluations\", json=nemo_single_evaluation)\n",
        "\n",
        "if response.status_code == 202:\n",
        "    nemo_response = response.json()\n",
        "    nemo_request_id = nemo_response[\"request_id\"]\n",
        "    print(f\"✅ NeMo evaluation created!\")\n",
        "    print(f\"Request ID: {nemo_request_id}\")\n",
        "else:\n",
        "    print(\"⚠️ NeMo evaluation failed (container may not be running)\")\n",
        "    print(f\"Response: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Container NeMo Evaluator Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with multiple specialized NeMo Evaluator containers\n",
        "nemo_multi_evaluation = {\n",
        "    \"request_id\": str(uuid4()),\n",
        "    \"experiment_name\": \"Multi-Container NeMo Evaluation\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"Distributed LLaMA Evaluation\",\n",
        "            \"description\": \"Multi-container evaluation across specialized endpoints\",\n",
        "            \"model\": {\n",
        "                \"server\": \"default\",\n",
        "                \"name\": \"default\"\n",
        "            },\n",
        "            \"model_configuration\": {\n",
        "                \"temperature\": 0.1,\n",
        "                \"max_tokens\": 512,\n",
        "                \"top_p\": 0.95\n",
        "            },\n",
        "            \"backends\": [\n",
        "                {\n",
        "                    \"name\": \"academic-evaluator\",\n",
        "                    \"type\": \"nemo-evaluator\",\n",
        "                    \"config\": {\n",
        "                        \"endpoint\": \"academic-eval.example.com\",\n",
        "                        \"port\": 3825,\n",
        "                        \"model_endpoint\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                        \"endpoint_type\": \"chat\",\n",
        "                        \"api_key_env\": \"GROQ_API_KEY\",\n",
        "                        \"timeout_seconds\": 3600,\n",
        "                        \"framework_name\": \"eval-hub-academic\",\n",
        "                        \"parallelism\": 2\n",
        "                    },\n",
        "                    \"benchmarks\": [\n",
        "                        {\n",
        "                            \"name\": \"mmlu_pro\",\n",
        "                            \"tasks\": [\"mmlu_pro\"],\n",
        "                            \"config\": {\"limit\": 100, \"num_fewshot\": 5}\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"arc_challenge\",\n",
        "                            \"tasks\": [\"arc_challenge\"],\n",
        "                            \"config\": {\"limit\": 200, \"num_fewshot\": 25}\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"math-evaluator\",\n",
        "                    \"type\": \"nemo-evaluator\",\n",
        "                    \"config\": {\n",
        "                        \"endpoint\": \"math-eval.example.com\",\n",
        "                        \"port\": 3825,\n",
        "                        \"model_endpoint\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                        \"endpoint_type\": \"chat\",\n",
        "                        \"api_key_env\": \"GROQ_API_KEY\",\n",
        "                        \"temperature\": 0.0,\n",
        "                        \"parallelism\": 1,\n",
        "                        \"framework_name\": \"eval-hub-math\"\n",
        "                    },\n",
        "                    \"benchmarks\": [\n",
        "                        {\n",
        "                            \"name\": \"gsm8k\",\n",
        "                            \"tasks\": [\"gsm8k\"],\n",
        "                            \"config\": {\"limit\": 100, \"num_fewshot\": 8}\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"math\",\n",
        "                            \"tasks\": [\"hendrycks_math\"],\n",
        "                            \"config\": {\"limit\": 50, \"num_fewshot\": 4}\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            \"timeout_minutes\": 120,\n",
        "            \"retry_attempts\": 2\n",
        "        }\n",
        "    ],\n",
        "    \"tags\": {\n",
        "        \"example_type\": \"nemo_evaluator_multi\",\n",
        "        \"complexity\": \"expert\",\n",
        "        \"backend\": \"distributed_containers\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"📝 Creating multi-container NeMo evaluation...\")\n",
        "print(\"Note: This is a hypothetical example with multiple remote containers\")\n",
        "print_json(nemo_multi_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Status Monitoring\n",
        "\n",
        "### Check Evaluation Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to check evaluation status\n",
        "def check_evaluation_status(request_id: str):\n",
        "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        status_data = response.json()\n",
        "        print(f\"📊 Evaluation Status for {request_id}\")\n",
        "        print(f\"Status: {status_data['status']}\")\n",
        "        print(f\"Progress: {status_data.get('progress_percentage', 0):.1f}%\")\n",
        "        print(f\"Total evaluations: {status_data.get('total_evaluations', 0)}\")\n",
        "        print(f\"Completed: {status_data.get('completed_evaluations', 0)}\")\n",
        "        print(f\"Failed: {status_data.get('failed_evaluations', 0)}\")\n",
        "\n",
        "        if status_data.get('results'):\n",
        "            print(f\"Results available: {len(status_data['results'])}\")\n",
        "\n",
        "        return status_data\n",
        "    else:\n",
        "        print(f\"❌ Failed to get status: {response.text}\")\n",
        "        return None\n",
        "\n",
        "# Check status of previously created evaluations (if they exist)\n",
        "try:\n",
        "    if 'request_id' in locals():\n",
        "        check_evaluation_status(request_id)\n",
        "except NameError:\n",
        "    print(\"No evaluation request_id available to check\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitor Evaluation Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to monitor evaluation until completion\n",
        "def monitor_evaluation(request_id: str, max_wait_time: int = 300):\n",
        "    \"\"\"Monitor an evaluation until completion or timeout.\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    while time.time() - start_time < max_wait_time:\n",
        "        status_data = check_evaluation_status(request_id)\n",
        "\n",
        "        if not status_data:\n",
        "            break\n",
        "\n",
        "        status = status_data['status']\n",
        "\n",
        "        if status in ['completed', 'failed', 'cancelled']:\n",
        "            print(f\"🏁 Evaluation {status}!\")\n",
        "\n",
        "            if status == 'completed' and status_data.get('results'):\n",
        "                print(\"\\n📊 Results Summary:\")\n",
        "                for result in status_data['results'][:3]:  # Show first 3 results\n",
        "                    print(f\"  - {result['benchmark_name']}: {result['status']}\")\n",
        "                    if result.get('metrics'):\n",
        "                        for metric, value in list(result['metrics'].items())[:2]:\n",
        "                            print(f\"    {metric}: {value}\")\n",
        "\n",
        "            return status_data\n",
        "\n",
        "        print(f\"⏳ Still {status}, waiting...\")\n",
        "        time.sleep(10)\n",
        "\n",
        "    print(f\"⏰ Monitoring timed out after {max_wait_time} seconds\")\n",
        "    return None\n",
        "\n",
        "# Example usage (uncomment if you have a running evaluation)\n",
        "# monitor_evaluation(request_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List All Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = api_request(\"GET\", \"/evaluations\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    evaluations = response.json()\n",
        "    print(f\"📋 Active evaluations: {len(evaluations)}\")\n",
        "\n",
        "    for eval_resp in evaluations:\n",
        "        print(f\"\\n🔍 {eval_resp['request_id']}\")\n",
        "        print(f\"   Status: {eval_resp['status']}\")\n",
        "        print(f\"   Progress: {eval_resp.get('progress_percentage', 0):.1f}%\")\n",
        "        print(f\"   Created: {eval_resp['created_at']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## System Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = api_request(\"GET\", \"/metrics/system\")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    metrics = response.json()\n",
        "    print(\"📊 System Metrics:\")\n",
        "    print(f\"  Active evaluations: {metrics['active_evaluations']}\")\n",
        "    print(f\"  Running tasks: {metrics['running_tasks']}\")\n",
        "    print(f\"  Total requests: {metrics['total_requests']}\")\n",
        "\n",
        "    if metrics.get('status_breakdown'):\n",
        "        print(\"\\n  Status breakdown:\")\n",
        "        for status, count in metrics['status_breakdown'].items():\n",
        "            print(f\"    {status}: {count}\")\n",
        "\n",
        "    if metrics.get('memory_usage'):\n",
        "        print(f\"\\n  Memory usage:\")\n",
        "        print(f\"    Active evaluations: {metrics['memory_usage']['active_evaluations_mb']:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Management\n",
        "\n",
        "### Cancel an Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to cancel an evaluation\n",
        "def cancel_evaluation(request_id: str):\n",
        "    response = api_request(\"DELETE\", f\"/evaluations/{request_id}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(f\"✅ {result['message']}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"❌ Failed to cancel: {response.text}\")\n",
        "        return False\n",
        "\n",
        "# Example usage (uncomment if you want to cancel an evaluation)\n",
        "# cancel_evaluation(request_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling Examples\n",
        "\n",
        "### Invalid Request Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of invalid request to demonstrate error handling\n",
        "invalid_request = {\n",
        "    \"request_id\": \"invalid-uuid-format\",\n",
        "    \"evaluations\": [\n",
        "        {\n",
        "            \"name\": \"\",  # Invalid: empty name\n",
        "            \"model\": {\n",
        "                \"server\": \"\",  # Invalid: empty server\n",
        "                \"name\": \"\"  # Invalid: empty model name\n",
        "            },\n",
        "            \"backends\": []  # Invalid: no backends\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"📝 Testing error handling with invalid request...\")\n",
        "response = api_request(\"POST\", \"/evaluations\", json=invalid_request)\n",
        "\n",
        "if response.status_code >= 400:\n",
        "    print(\"✅ Error handling working correctly\")\n",
        "    error_data = response.json()\n",
        "    print(f\"Error type: {response.status_code}\")\n",
        "    print(f\"Error message: {error_data.get('detail', 'Unknown error')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Non-existent Resource Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test accessing non-existent evaluation\n",
        "fake_request_id = str(uuid4())\n",
        "print(f\"🔍 Testing access to non-existent evaluation: {fake_request_id}\")\n",
        "\n",
        "response = api_request(\"GET\", f\"/evaluations/{fake_request_id}\")\n",
        "\n",
        "if response.status_code == 404:\n",
        "    print(\"✅ 404 handling working correctly\")\n",
        "    error_data = response.json()\n",
        "    print(f\"Error: {error_data['detail']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Examples\n",
        "\n",
        "### Batch Evaluation Requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create multiple evaluations for comparison\n",
        "batch_requests = []\n",
        "\n",
        "models_to_compare = [\"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
        "risk_levels = [\"low\", \"medium\"]\n",
        "\n",
        "for model in models_to_compare:\n",
        "    for risk in risk_levels:\n",
        "        batch_request = {\n",
        "            \"request_id\": str(uuid4()),\n",
        "            \"experiment_name\": f\"Batch Comparison - {model} - {risk} risk\",\n",
        "            \"evaluations\": [\n",
        "                {\n",
        "                    \"name\": f\"{model} {risk} risk evaluation\",\n",
        "                    \"model\": {\n",
        "                        \"server\": \"default\",\n",
        "                        \"name\": \"default\"\n",
        "                    },\n",
        "                    \"model_configuration\": {\n",
        "                        \"temperature\": 0.0,\n",
        "                        \"max_tokens\": 256\n",
        "                    },\n",
        "                    \"risk_category\": risk,\n",
        "                    \"timeout_minutes\": 30\n",
        "                }\n",
        "            ],\n",
        "            \"tags\": {\n",
        "                \"batch_id\": \"model_comparison_001\",\n",
        "                \"model\": model,\n",
        "                \"risk_level\": risk\n",
        "            }\n",
        "        }\n",
        "        batch_requests.append(batch_request)\n",
        "\n",
        "print(f\"📦 Creating {len(batch_requests)} batch evaluations...\")\n",
        "\n",
        "batch_results = []\n",
        "for i, request in enumerate(batch_requests):\n",
        "    print(f\"\\n📝 Creating batch request {i+1}/{len(batch_requests)}\")\n",
        "    response = api_request(\"POST\", \"/evaluations\", json=request)\n",
        "\n",
        "    if response.status_code == 202:\n",
        "        batch_results.append(response.json())\n",
        "        print(f\"✅ Batch {i+1} created: {response.json()['request_id']}\")\n",
        "    else:\n",
        "        print(f\"❌ Batch {i+1} failed\")\n",
        "\n",
        "print(f\"\\n📊 Successfully created {len(batch_results)} batch evaluations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test various configuration combinations\n",
        "test_configs = [\n",
        "    {\n",
        "        \"name\": \"High timeout test\",\n",
        "        \"config\": {\"timeout_minutes\": 120, \"retry_attempts\": 5},\n",
        "        \"expected\": \"success\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Zero timeout test\",\n",
        "        \"config\": {\"timeout_minutes\": 0, \"retry_attempts\": 1},\n",
        "        \"expected\": \"validation_error\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Negative retry test\",\n",
        "        \"config\": {\"timeout_minutes\": 30, \"retry_attempts\": -1},\n",
        "        \"expected\": \"validation_error\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for test in test_configs:\n",
        "    print(f\"\\n🧪 Testing: {test['name']}\")\n",
        "\n",
        "    test_request = {\n",
        "        \"request_id\": str(uuid4()),\n",
        "        \"experiment_name\": test['name'],\n",
        "        \"evaluations\": [\n",
        "            {\n",
        "                \"name\": \"Config test\",\n",
        "                \"model\": {\n",
        "                    \"server\": \"default\",\n",
        "                    \"name\": \"default\"\n",
        "                },\n",
        "                \"risk_category\": \"low\",\n",
        "                **test['config']\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = api_request(\"POST\", \"/evaluations\", json=test_request)\n",
        "\n",
        "    if test['expected'] == \"success\" and response.status_code == 202:\n",
        "        print(\"✅ Test passed\")\n",
        "    elif test['expected'] == \"validation_error\" and response.status_code >= 400:\n",
        "        print(\"✅ Validation correctly rejected invalid config\")\n",
        "    else:\n",
        "        print(f\"❌ Unexpected result: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated comprehensive usage of the Eval Hub API including:\n",
        "\n",
        "- ✅ **Basic Operations**: Health checks, provider/benchmark discovery\n",
        "- ✅ **Model Management**: Register, list, update, and delete models\n",
        "- ✅ **Simple Evaluations**: Risk category-based evaluations\n",
        "- ✅ **Advanced Evaluations**: Explicit backend configuration\n",
        "- ✅ **NeMo Integration**: Single and multi-container setups\n",
        "- ✅ **Monitoring**: Status checking and progress tracking\n",
        "- ✅ **Management**: Cancellation and system metrics\n",
        "- ✅ **Error Handling**: Validation and error responses\n",
        "- ✅ **Batch Operations**: Multiple evaluation management\n",
        "\n",
        "For production use, remember to:\n",
        "- Use proper API keys and authentication\n",
        "- Configure appropriate timeouts for your evaluation complexity\n",
        "- Monitor resource usage and system metrics\n",
        "- Handle errors gracefully in your applications\n",
        "- Use the async evaluation mode for long-running evaluations\n",
        "\n",
        "The Eval Hub provides a powerful and flexible API for orchestrating machine learning model evaluations across multiple backends and evaluation frameworks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
